
Random seeds have been fixed as 42


Random seeds have been fixed as 42


 ---------------------------------------- fold#0 starts. ---------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Conv2d: 1-1                            [-1, 40, 1, 97]           1,000
├─Mish: 1-2                              [-1, 40, 1, 97]           --
├─Conv2d: 1-3                            [-1, 40, 1, 94]           6,440
├─BatchNorm2d: 1-4                       [-1, 40, 1, 94]           80
├─MaxPool2d: 1-5                         [-1, 40, 1, 18]           --
├─Mish: 1-6                              [-1, 40, 1, 18]           --
├─SwapLayer: 1-7                         [-1, 1, 40, 18]           --
├─Conv2d: 1-8                            [-1, 50, 33, 7]           4,850
├─BatchNorm2d: 1-9                       [-1, 50, 33, 7]           100
├─MaxPool2d: 1-10                        [-1, 50, 11, 2]           --
├─Mish: 1-11                             [-1, 50, 11, 2]           --
├─Conv2d: 1-12                           [-1, 50, 9, 2]            7,550
├─BatchNorm2d: 1-13                      [-1, 50, 9, 2]            100
├─MaxPool2d: 1-14                        [-1, 50, 9, 1]            --
├─Mish: 1-15                             [-1, 50, 9, 1]            --
├─Sequential: 1-16                       [-1, 2]                   --
|    └─Linear: 2-1                       [-1, 1024]                461,824
|    └─Mish: 2-2                         [-1, 1024]                --
|    └─Dropout: 2-3                      [-1, 1024]                --
|    └─Linear: 2-4                       [-1, 256]                 262,400
|    └─Mish: 2-5                         [-1, 256]                 --
|    └─Dropout: 2-6                      [-1, 256]                 --
|    └─Linear: 2-7                       [-1, 2]                   514
├─Sequential: 1-17                       [-1, 3]                   --
|    └─Linear: 2-8                       [-1, 1024]                461,824
|    └─Mish: 2-9                         [-1, 1024]                --
|    └─Dropout: 2-10                     [-1, 1024]                --
|    └─Linear: 2-11                      [-1, 256]                 262,400
|    └─Mish: 2-12                        [-1, 256]                 --
|    └─Dropout: 2-13                     [-1, 256]                 --
|    └─Linear: 2-14                      [-1, 3]                   771
==========================================================================================
Total params: 1,469,853
Trainable params: 1,469,853
Non-trainable params: 0
Total mult-adds (M): 4.83
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 0.30
Params size (MB): 5.61
Estimated Total Size (MB): 6.05
==========================================================================================

----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0    0.34333        0.0   0.34333

----------------------------------------

==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Conv2d: 1-1                            [-1, 40, 1, 97]           1,000
├─Mish: 1-2                              [-1, 40, 1, 97]           --
├─Conv2d: 1-3                            [-1, 40, 1, 94]           6,440
├─BatchNorm2d: 1-4                       [-1, 40, 1, 94]           80
├─MaxPool2d: 1-5                         [-1, 40, 1, 18]           --
├─Mish: 1-6                              [-1, 40, 1, 18]           --
├─SwapLayer: 1-7                         [-1, 1, 40, 18]           --
├─Conv2d: 1-8                            [-1, 50, 33, 7]           4,850
├─BatchNorm2d: 1-9                       [-1, 50, 33, 7]           100
├─MaxPool2d: 1-10                        [-1, 50, 11, 2]           --
├─Mish: 1-11                             [-1, 50, 11, 2]           --
├─Conv2d: 1-12                           [-1, 50, 9, 2]            7,550
├─BatchNorm2d: 1-13                      [-1, 50, 9, 2]            100
├─MaxPool2d: 1-14                        [-1, 50, 9, 1]            --
├─Mish: 1-15                             [-1, 50, 9, 1]            --
├─Sequential: 1-16                       [-1, 2]                   --
|    └─Linear: 2-1                       [-1, 1024]                461,824
|    └─Mish: 2-2                         [-1, 1024]                --
|    └─Dropout: 2-3                      [-1, 1024]                --
|    └─Linear: 2-4                       [-1, 256]                 262,400
|    └─Mish: 2-5                         [-1, 256]                 --
|    └─Dropout: 2-6                      [-1, 256]                 --
|    └─Linear: 2-7                       [-1, 2]                   514
├─Sequential: 1-17                       [-1, 3]                   --
|    └─Linear: 2-8                       [-1, 1024]                461,824
|    └─Mish: 2-9                         [-1, 1024]                --
|    └─Dropout: 2-10                     [-1, 1024]                --
|    └─Linear: 2-11                      [-1, 256]                 262,400
|    └─Mish: 2-12                        [-1, 256]                 --
|    └─Dropout: 2-13                     [-1, 256]                 --
|    └─Linear: 2-14                      [-1, 3]                   771
==========================================================================================
Total params: 1,469,853
Trainable params: 1,469,853
Non-trainable params: 0
Total mult-adds (M): 4.83
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 0.30
Params size (MB): 5.61
Estimated Total Size (MB): 6.05
==========================================================================================

----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595    0.39951     0.0      33.5   0.731526   0.796334  1.526776

----------------------------------------


Validation score decreased (inf --> -0.504059).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#0_epoch#000.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#0_epoch#000.pth

remove: path should be string, bytes or os.PathLike, not NoneType

----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006

----------------------------------------


Validation score decreased (-0.504059 --> -0.522171).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#0_epoch#001.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#0_epoch#001.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#0_epoch#000.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#0_epoch#000.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297

----------------------------------------


Validation score decreased (-0.522171 --> -0.560527).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#0_epoch#002.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#0_epoch#002.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#0_epoch#001.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#0_epoch#001.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087

----------------------------------------


Validation score decreased (-0.560527 --> -0.580501).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#0_epoch#003.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#0_epoch#003.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#0_epoch#002.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#0_epoch#002.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330

----------------------------------------


Validation score decreased (-0.580501 --> -0.597176).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#0_epoch#004.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#0_epoch#004.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#0_epoch#003.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#0_epoch#003.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293

----------------------------------------


Validation score decreased (-0.597176 --> -0.623118).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#0_epoch#005.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#0_epoch#005.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#0_epoch#004.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#0_epoch#004.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359

----------------------------------------


EarlyStopping counter: 2 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895

----------------------------------------


EarlyStopping counter: 3 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985
9         0.602935        1.0     0.0     612.0   0.405582        0.0  0.362181

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895
9         0.579752   1.000000     0.0     645.5   0.660829   0.000659  0.591419

----------------------------------------


EarlyStopping counter: 4 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985
9         0.602935        1.0     0.0     612.0   0.405582        0.0  0.362181
10        0.578165        1.0     0.0     680.0   0.405235        0.0  0.353683

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895
9         0.579752   1.000000     0.0     645.5   0.660829   0.000659  0.591419
10        0.570984   1.000000     0.0     713.5   0.663307   0.000559  0.582652

----------------------------------------


EarlyStopping counter: 5 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985
9         0.602935        1.0     0.0     612.0   0.405582        0.0  0.362181
10        0.578165        1.0     0.0     680.0   0.405235        0.0  0.353683
11        0.573019        1.0     0.0     748.0   0.390029        0.0  0.331597

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895
9         0.579752   1.000000     0.0     645.5   0.660829   0.000659  0.591419
10        0.570984   1.000000     0.0     713.5   0.663307   0.000559  0.582652
11        0.558076   1.000000     0.0     781.5   0.658760   0.000420  0.567251

----------------------------------------


EarlyStopping counter: 6 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985
9         0.602935        1.0     0.0     612.0   0.405582        0.0  0.362181
10        0.578165        1.0     0.0     680.0   0.405235        0.0  0.353683
11        0.573019        1.0     0.0     748.0   0.390029        0.0  0.331597
12        0.576508        1.0     0.0     816.0   0.395137        0.0  0.328188

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895
9         0.579752   1.000000     0.0     645.5   0.660829   0.000659  0.591419
10        0.570984   1.000000     0.0     713.5   0.663307   0.000559  0.582652
11        0.558076   1.000000     0.0     781.5   0.658760   0.000420  0.567251
12        0.571893   1.000000     0.0     849.5   0.658021   0.000272  0.555084

----------------------------------------


EarlyStopping counter: 7 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985
9         0.602935        1.0     0.0     612.0   0.405582        0.0  0.362181
10        0.578165        1.0     0.0     680.0   0.405235        0.0  0.353683
11        0.573019        1.0     0.0     748.0   0.390029        0.0  0.331597
12        0.576508        1.0     0.0     816.0   0.395137        0.0  0.328188
13        0.575254        1.0     0.0     884.0   0.400978        0.0  0.324265

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895
9         0.579752   1.000000     0.0     645.5   0.660829   0.000659  0.591419
10        0.570984   1.000000     0.0     713.5   0.663307   0.000559  0.582652
11        0.558076   1.000000     0.0     781.5   0.658760   0.000420  0.567251
12        0.571893   1.000000     0.0     849.5   0.658021   0.000272  0.555084
13        0.585463   1.000000     0.0     917.5   0.649689   0.000242  0.536231

----------------------------------------


EarlyStopping counter: 8 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985
9         0.602935        1.0     0.0     612.0   0.405582        0.0  0.362181
10        0.578165        1.0     0.0     680.0   0.405235        0.0  0.353683
11        0.573019        1.0     0.0     748.0   0.390029        0.0  0.331597
12        0.576508        1.0     0.0     816.0   0.395137        0.0  0.328188
13        0.575254        1.0     0.0     884.0   0.400978        0.0  0.324265
14        0.577729        1.0     0.0     952.0   0.404038        0.0  0.317351

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895
9         0.579752   1.000000     0.0     645.5   0.660829   0.000659  0.591419
10        0.570984   1.000000     0.0     713.5   0.663307   0.000559  0.582652
11        0.558076   1.000000     0.0     781.5   0.658760   0.000420  0.567251
12        0.571893   1.000000     0.0     849.5   0.658021   0.000272  0.555084
13        0.585463   1.000000     0.0     917.5   0.649689   0.000242  0.536231
14        0.578960   1.000000     0.0     985.5   0.656255   0.000227  0.530177

----------------------------------------


EarlyStopping counter: 9 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985
9         0.602935        1.0     0.0     612.0   0.405582        0.0  0.362181
10        0.578165        1.0     0.0     680.0   0.405235        0.0  0.353683
11        0.573019        1.0     0.0     748.0   0.390029        0.0  0.331597
12        0.576508        1.0     0.0     816.0   0.395137        0.0  0.328188
13        0.575254        1.0     0.0     884.0   0.400978        0.0  0.324265
14        0.577729        1.0     0.0     952.0   0.404038        0.0  0.317351
15        0.561911        1.0     0.0    1020.0   0.392342        0.0  0.298353

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895
9         0.579752   1.000000     0.0     645.5   0.660829   0.000659  0.591419
10        0.570984   1.000000     0.0     713.5   0.663307   0.000559  0.582652
11        0.558076   1.000000     0.0     781.5   0.658760   0.000420  0.567251
12        0.571893   1.000000     0.0     849.5   0.658021   0.000272  0.555084
13        0.585463   1.000000     0.0     917.5   0.649689   0.000242  0.536231
14        0.578960   1.000000     0.0     985.5   0.656255   0.000227  0.530177
15        0.593840   1.000000     0.0    1053.5   0.651645   0.000203  0.514289

----------------------------------------


EarlyStopping counter: 10 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985
9         0.602935        1.0     0.0     612.0   0.405582        0.0  0.362181
10        0.578165        1.0     0.0     680.0   0.405235        0.0  0.353683
11        0.573019        1.0     0.0     748.0   0.390029        0.0  0.331597
12        0.576508        1.0     0.0     816.0   0.395137        0.0  0.328188
13        0.575254        1.0     0.0     884.0   0.400978        0.0  0.324265
14        0.577729        1.0     0.0     952.0   0.404038        0.0  0.317351
15        0.561911        1.0     0.0    1020.0   0.392342        0.0  0.298353
16        0.547772        1.0     0.0    1088.0   0.392238        0.0  0.288229

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895
9         0.579752   1.000000     0.0     645.5   0.660829   0.000659  0.591419
10        0.570984   1.000000     0.0     713.5   0.663307   0.000559  0.582652
11        0.558076   1.000000     0.0     781.5   0.658760   0.000420  0.567251
12        0.571893   1.000000     0.0     849.5   0.658021   0.000272  0.555084
13        0.585463   1.000000     0.0     917.5   0.649689   0.000242  0.536231
14        0.578960   1.000000     0.0     985.5   0.656255   0.000227  0.530177
15        0.593840   1.000000     0.0    1053.5   0.651645   0.000203  0.514289
16        0.591409   1.000000     0.0    1121.5   0.644537   0.000195  0.496199

----------------------------------------


EarlyStopping counter: 11 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985
9         0.602935        1.0     0.0     612.0   0.405582        0.0  0.362181
10        0.578165        1.0     0.0     680.0   0.405235        0.0  0.353683
11        0.573019        1.0     0.0     748.0   0.390029        0.0  0.331597
12        0.576508        1.0     0.0     816.0   0.395137        0.0  0.328188
13        0.575254        1.0     0.0     884.0   0.400978        0.0  0.324265
14        0.577729        1.0     0.0     952.0   0.404038        0.0  0.317351
15        0.561911        1.0     0.0    1020.0   0.392342        0.0  0.298353
16        0.547772        1.0     0.0    1088.0   0.392238        0.0  0.288229
17        0.557392        1.0     0.0    1156.0   0.401542        0.0  0.285490

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895
9         0.579752   1.000000     0.0     645.5   0.660829   0.000659  0.591419
10        0.570984   1.000000     0.0     713.5   0.663307   0.000559  0.582652
11        0.558076   1.000000     0.0     781.5   0.658760   0.000420  0.567251
12        0.571893   1.000000     0.0     849.5   0.658021   0.000272  0.555084
13        0.585463   1.000000     0.0     917.5   0.649689   0.000242  0.536231
14        0.578960   1.000000     0.0     985.5   0.656255   0.000227  0.530177
15        0.593840   1.000000     0.0    1053.5   0.651645   0.000203  0.514289
16        0.591409   1.000000     0.0    1121.5   0.644537   0.000195  0.496199
17        0.582440   1.000000     0.0    1189.5   0.646801   0.000208  0.485913

----------------------------------------


EarlyStopping counter: 12 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985
9         0.602935        1.0     0.0     612.0   0.405582        0.0  0.362181
10        0.578165        1.0     0.0     680.0   0.405235        0.0  0.353683
11        0.573019        1.0     0.0     748.0   0.390029        0.0  0.331597
12        0.576508        1.0     0.0     816.0   0.395137        0.0  0.328188
13        0.575254        1.0     0.0     884.0   0.400978        0.0  0.324265
14        0.577729        1.0     0.0     952.0   0.404038        0.0  0.317351
15        0.561911        1.0     0.0    1020.0   0.392342        0.0  0.298353
16        0.547772        1.0     0.0    1088.0   0.392238        0.0  0.288229
17        0.557392        1.0     0.0    1156.0   0.401542        0.0  0.285490
18        0.557269        1.0     0.0    1224.0   0.397545        0.0  0.272188

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895
9         0.579752   1.000000     0.0     645.5   0.660829   0.000659  0.591419
10        0.570984   1.000000     0.0     713.5   0.663307   0.000559  0.582652
11        0.558076   1.000000     0.0     781.5   0.658760   0.000420  0.567251
12        0.571893   1.000000     0.0     849.5   0.658021   0.000272  0.555084
13        0.585463   1.000000     0.0     917.5   0.649689   0.000242  0.536231
14        0.578960   1.000000     0.0     985.5   0.656255   0.000227  0.530177
15        0.593840   1.000000     0.0    1053.5   0.651645   0.000203  0.514289
16        0.591409   1.000000     0.0    1121.5   0.644537   0.000195  0.496199
17        0.582440   1.000000     0.0    1189.5   0.646801   0.000208  0.485913
18        0.598916   1.000000     0.0    1257.5   0.640082   0.000138  0.467876

----------------------------------------


EarlyStopping counter: 13 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.504059        1.0     0.0       0.0   0.343330        0.0  0.343330
1         0.522171        1.0     0.0      68.0   0.363551        0.0  0.366265
2         0.560527        1.0     0.0     136.0   0.386638        0.0  0.387381
3         0.580501        1.0     0.0     204.0   0.391154        0.0  0.387533
4         0.597176        1.0     0.0     272.0   0.389487        0.0  0.380422
5         0.623118        1.0     0.0     340.0   0.390439        0.0  0.375302
6         0.614277        1.0     0.0     408.0   0.393741        0.0  0.372548
7         0.595984        1.0     0.0     476.0   0.392827        0.0  0.364828
8         0.587344        1.0     0.0     544.0   0.392230        0.0  0.356985
9         0.602935        1.0     0.0     612.0   0.405582        0.0  0.362181
10        0.578165        1.0     0.0     680.0   0.405235        0.0  0.353683
11        0.573019        1.0     0.0     748.0   0.390029        0.0  0.331597
12        0.576508        1.0     0.0     816.0   0.395137        0.0  0.328188
13        0.575254        1.0     0.0     884.0   0.400978        0.0  0.324265
14        0.577729        1.0     0.0     952.0   0.404038        0.0  0.317351
15        0.561911        1.0     0.0    1020.0   0.392342        0.0  0.298353
16        0.547772        1.0     0.0    1088.0   0.392238        0.0  0.288229
17        0.557392        1.0     0.0    1156.0   0.401542        0.0  0.285490
18        0.557269        1.0     0.0    1224.0   0.397545        0.0  0.272188
19        0.574837        1.0     0.0    1292.0   0.391994        0.0  0.256283

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.501595   0.399510     0.0      33.5   0.731526   0.796334  1.526776
1         0.537996   0.985294     0.0     101.5   0.693151   0.145475  0.838006
2         0.519819   1.000000     0.0     169.5   0.697137   0.022026  0.713297
3         0.534548   1.000000     0.0     237.5   0.685891   0.007543  0.680087
4         0.556200   1.000000     0.0     305.5   0.674908   0.003937  0.657330
5         0.537661   1.000000     0.0     373.5   0.677751   0.002257  0.649293
6         0.559383   1.000000     0.0     441.5   0.671867   0.001550  0.633408
7         0.565767   1.000000     0.0     509.5   0.669006   0.001196  0.620359
8         0.555847   1.000000     0.0     577.5   0.657322   0.000845  0.598895
9         0.579752   1.000000     0.0     645.5   0.660829   0.000659  0.591419
10        0.570984   1.000000     0.0     713.5   0.663307   0.000559  0.582652
11        0.558076   1.000000     0.0     781.5   0.658760   0.000420  0.567251
12        0.571893   1.000000     0.0     849.5   0.658021   0.000272  0.555084
13        0.585463   1.000000     0.0     917.5   0.649689   0.000242  0.536231
14        0.578960   1.000000     0.0     985.5   0.656255   0.000227  0.530177
15        0.593840   1.000000     0.0    1053.5   0.651645   0.000203  0.514289
16        0.591409   1.000000     0.0    1121.5   0.644537   0.000195  0.496199
17        0.582440   1.000000     0.0    1189.5   0.646801   0.000208  0.485913
18        0.598916   1.000000     0.0    1257.5   0.640082   0.000138  0.467876
19        0.583303   1.000000     0.0    1325.5   0.644006   0.000156  0.458607

----------------------------------------


EarlyStopping counter: 14 out of 50


----------------------------------------


Test: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
5         0.571042        1.0     0.0     408.0   0.697226        0.0   0.66101

----------------------------------------


Matplotilb has been configured as follows:
{'figure.dpi': 100, 'savefig.dpi': 300, 'figure.figsize': '(7.0, 7.0) [cm]', 'font.size': 7, 'axes.labelsize': 8, 'xtick.labelsize': 8, 'ytick.labelsize': 8, 'axes.titlesize': 8, 'legend.fontsize': 6, 'pdf.fonttype': 42, 'ps.fonttype': 42, 'axes.spines.top': True, 'axes.spines.right': True, 'xtick.major.size': 2.032, 'ytick.major.size': 2.032, 'xtick.major.width': 0.508, 'ytick.major.width': 0.508}.


Balanced ACC in fold#0 was 0.501


MCC in fold#0 was 0.009


Confusion Matrix in fold#0: 

            NoN-Ripple  Ripple
NoN-Ripple        2754      11
Ripple            1897      10


Classification Report for fold#0:

             NoN-Ripple    Ripple  balanced accuracy  macro avg  weighted avg
precision         0.592     0.476              0.501      0.534         0.545
recall            0.996     0.005              0.501      0.501         0.592
f1-score          0.743     0.010              0.501      0.377         0.444
sample size    2765.000  1907.000              0.501   4672.000      4672.000


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/fold#0/NoN-Ripple.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/fold#0/Ripple.csv


Matplotilb has been configured as follows:
{'figure.dpi': 300, 'savefig.dpi': 300, 'figure.figsize': '(32.4, 20.0) [cm]', 'font.size': 16, 'axes.labelsize': 16, 'xtick.labelsize': 16, 'ytick.labelsize': 16, 'axes.titlesize': 16, 'legend.fontsize': 'xx-small', 'pdf.fonttype': 42, 'ps.fonttype': 42, 'axes.spines.top': True, 'axes.spines.right': True}.


 ---------------------------------------- fold#1 starts. ---------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Conv2d: 1-1                            [-1, 40, 1, 97]           1,000
├─Mish: 1-2                              [-1, 40, 1, 97]           --
├─Conv2d: 1-3                            [-1, 40, 1, 94]           6,440
├─BatchNorm2d: 1-4                       [-1, 40, 1, 94]           80
├─MaxPool2d: 1-5                         [-1, 40, 1, 18]           --
├─Mish: 1-6                              [-1, 40, 1, 18]           --
├─SwapLayer: 1-7                         [-1, 1, 40, 18]           --
├─Conv2d: 1-8                            [-1, 50, 33, 7]           4,850
├─BatchNorm2d: 1-9                       [-1, 50, 33, 7]           100
├─MaxPool2d: 1-10                        [-1, 50, 11, 2]           --
├─Mish: 1-11                             [-1, 50, 11, 2]           --
├─Conv2d: 1-12                           [-1, 50, 9, 2]            7,550
├─BatchNorm2d: 1-13                      [-1, 50, 9, 2]            100
├─MaxPool2d: 1-14                        [-1, 50, 9, 1]            --
├─Mish: 1-15                             [-1, 50, 9, 1]            --
├─Sequential: 1-16                       [-1, 2]                   --
|    └─Linear: 2-1                       [-1, 1024]                461,824
|    └─Mish: 2-2                         [-1, 1024]                --
|    └─Dropout: 2-3                      [-1, 1024]                --
|    └─Linear: 2-4                       [-1, 256]                 262,400
|    └─Mish: 2-5                         [-1, 256]                 --
|    └─Dropout: 2-6                      [-1, 256]                 --
|    └─Linear: 2-7                       [-1, 2]                   514
├─Sequential: 1-17                       [-1, 3]                   --
|    └─Linear: 2-8                       [-1, 1024]                461,824
|    └─Mish: 2-9                         [-1, 1024]                --
|    └─Dropout: 2-10                     [-1, 1024]                --
|    └─Linear: 2-11                      [-1, 256]                 262,400
|    └─Mish: 2-12                        [-1, 256]                 --
|    └─Dropout: 2-13                     [-1, 256]                 --
|    └─Linear: 2-14                      [-1, 3]                   771
==========================================================================================
Total params: 1,469,853
Trainable params: 1,469,853
Non-trainable params: 0
Total mult-adds (M): 4.83
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 0.30
Params size (MB): 5.61
Estimated Total Size (MB): 6.05
==========================================================================================

----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0    0.34856        0.0   0.34856

----------------------------------------

==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Conv2d: 1-1                            [-1, 40, 1, 97]           1,000
├─Mish: 1-2                              [-1, 40, 1, 97]           --
├─Conv2d: 1-3                            [-1, 40, 1, 94]           6,440
├─BatchNorm2d: 1-4                       [-1, 40, 1, 94]           80
├─MaxPool2d: 1-5                         [-1, 40, 1, 18]           --
├─Mish: 1-6                              [-1, 40, 1, 18]           --
├─SwapLayer: 1-7                         [-1, 1, 40, 18]           --
├─Conv2d: 1-8                            [-1, 50, 33, 7]           4,850
├─BatchNorm2d: 1-9                       [-1, 50, 33, 7]           100
├─MaxPool2d: 1-10                        [-1, 50, 11, 2]           --
├─Mish: 1-11                             [-1, 50, 11, 2]           --
├─Conv2d: 1-12                           [-1, 50, 9, 2]            7,550
├─BatchNorm2d: 1-13                      [-1, 50, 9, 2]            100
├─MaxPool2d: 1-14                        [-1, 50, 9, 1]            --
├─Mish: 1-15                             [-1, 50, 9, 1]            --
├─Sequential: 1-16                       [-1, 2]                   --
|    └─Linear: 2-1                       [-1, 1024]                461,824
|    └─Mish: 2-2                         [-1, 1024]                --
|    └─Dropout: 2-3                      [-1, 1024]                --
|    └─Linear: 2-4                       [-1, 256]                 262,400
|    └─Mish: 2-5                         [-1, 256]                 --
|    └─Dropout: 2-6                      [-1, 256]                 --
|    └─Linear: 2-7                       [-1, 2]                   514
├─Sequential: 1-17                       [-1, 3]                   --
|    └─Linear: 2-8                       [-1, 1024]                461,824
|    └─Mish: 2-9                         [-1, 1024]                --
|    └─Dropout: 2-10                     [-1, 1024]                --
|    └─Linear: 2-11                      [-1, 256]                 262,400
|    └─Mish: 2-12                        [-1, 256]                 --
|    └─Dropout: 2-13                     [-1, 256]                 --
|    └─Linear: 2-14                      [-1, 3]                   771
==========================================================================================
Total params: 1,469,853
Trainable params: 1,469,853
Non-trainable params: 0
Total mult-adds (M): 4.83
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 0.30
Params size (MB): 5.61
Estimated Total Size (MB): 6.05
==========================================================================================

----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069      0.375     1.0      33.5   0.737612    0.83234  1.568721

----------------------------------------


Validation score decreased (inf --> -0.521748).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#000.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#000.pth

remove: path should be string, bytes or os.PathLike, not NoneType

----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887

----------------------------------------


Validation score decreased (-0.521748 --> -0.558593).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#001.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#001.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#000.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#000.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366

----------------------------------------


EarlyStopping counter: 2 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088

----------------------------------------


EarlyStopping counter: 3 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468

----------------------------------------


EarlyStopping counter: 4 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522

----------------------------------------


Validation score decreased (-0.558593 --> -0.573515).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#006.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#006.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#001.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#001.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152

----------------------------------------


Validation score decreased (-0.573515 --> -0.600767).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#008.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#008.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#006.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#006.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430
9         0.616066        1.0     1.0     612.0   0.394421        0.0  0.352723

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152
9         0.570284   1.000000     1.0     645.5   0.661025   0.000693  0.592675

----------------------------------------


Validation score decreased (-0.600767 --> -0.616066).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#009.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#009.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#008.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#008.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430
9         0.616066        1.0     1.0     612.0   0.394421        0.0  0.352723
10        0.621134        1.0     1.0     680.0   0.400180        0.0  0.349939

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152
9         0.570284   1.000000     1.0     645.5   0.661025   0.000693  0.592675
10        0.582725   1.000000     1.0     713.5   0.657191   0.000488  0.578160

----------------------------------------


Validation score decreased (-0.616066 --> -0.621134).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#010.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#010.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#009.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#009.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430
9         0.616066        1.0     1.0     612.0   0.394421        0.0  0.352723
10        0.621134        1.0     1.0     680.0   0.400180        0.0  0.349939
11        0.576212        1.0     1.0     748.0   0.401411        0.0  0.342579

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152
9         0.570284   1.000000     1.0     645.5   0.661025   0.000693  0.592675
10        0.582725   1.000000     1.0     713.5   0.657191   0.000488  0.578160
11        0.563951   1.000000     1.0     781.5   0.656484   0.000457  0.566448

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430
9         0.616066        1.0     1.0     612.0   0.394421        0.0  0.352723
10        0.621134        1.0     1.0     680.0   0.400180        0.0  0.349939
11        0.576212        1.0     1.0     748.0   0.401411        0.0  0.342579
12        0.577153        1.0     1.0     816.0   0.392007        0.0  0.326218

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152
9         0.570284   1.000000     1.0     645.5   0.661025   0.000693  0.592675
10        0.582725   1.000000     1.0     713.5   0.657191   0.000488  0.578160
11        0.563951   1.000000     1.0     781.5   0.656484   0.000457  0.566448
12        0.578383   1.000000     1.0     852.0   0.672178   0.000457  0.568507

----------------------------------------


EarlyStopping counter: 2 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430
9         0.616066        1.0     1.0     612.0   0.394421        0.0  0.352723
10        0.621134        1.0     1.0     680.0   0.400180        0.0  0.349939
11        0.576212        1.0     1.0     748.0   0.401411        0.0  0.342579
12        0.577153        1.0     1.0     816.0   0.392007        0.0  0.326218
13        0.569557        1.0     1.0     889.0   0.366494        0.0  0.294607

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152
9         0.570284   1.000000     1.0     645.5   0.661025   0.000693  0.592675
10        0.582725   1.000000     1.0     713.5   0.657191   0.000488  0.578160
11        0.563951   1.000000     1.0     781.5   0.656484   0.000457  0.566448
12        0.578383   1.000000     1.0     852.0   0.672178   0.000457  0.568507
13        0.582134   1.000000     1.0     922.5   0.659761   0.000374  0.545384

----------------------------------------


EarlyStopping counter: 3 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430
9         0.616066        1.0     1.0     612.0   0.394421        0.0  0.352723
10        0.621134        1.0     1.0     680.0   0.400180        0.0  0.349939
11        0.576212        1.0     1.0     748.0   0.401411        0.0  0.342579
12        0.577153        1.0     1.0     816.0   0.392007        0.0  0.326218
13        0.569557        1.0     1.0     889.0   0.366494        0.0  0.294607
14        0.588229        1.0     1.0     957.0   0.381090        0.0  0.298231

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152
9         0.570284   1.000000     1.0     645.5   0.661025   0.000693  0.592675
10        0.582725   1.000000     1.0     713.5   0.657191   0.000488  0.578160
11        0.563951   1.000000     1.0     781.5   0.656484   0.000457  0.566448
12        0.578383   1.000000     1.0     852.0   0.672178   0.000457  0.568507
13        0.582134   1.000000     1.0     922.5   0.659761   0.000374  0.545384
14        0.578383   1.000000     1.0     990.5   0.649775   0.000271  0.524897

----------------------------------------


EarlyStopping counter: 4 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430
9         0.616066        1.0     1.0     612.0   0.394421        0.0  0.352723
10        0.621134        1.0     1.0     680.0   0.400180        0.0  0.349939
11        0.576212        1.0     1.0     748.0   0.401411        0.0  0.342579
12        0.577153        1.0     1.0     816.0   0.392007        0.0  0.326218
13        0.569557        1.0     1.0     889.0   0.366494        0.0  0.294607
14        0.588229        1.0     1.0     957.0   0.381090        0.0  0.298231
15        0.621343        1.0     1.0    1025.0   0.403029        0.0  0.307289

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152
9         0.570284   1.000000     1.0     645.5   0.661025   0.000693  0.592675
10        0.582725   1.000000     1.0     713.5   0.657191   0.000488  0.578160
11        0.563951   1.000000     1.0     781.5   0.656484   0.000457  0.566448
12        0.578383   1.000000     1.0     852.0   0.672178   0.000457  0.568507
13        0.582134   1.000000     1.0     922.5   0.659761   0.000374  0.545384
14        0.578383   1.000000     1.0     990.5   0.649775   0.000271  0.524897
15        0.584468   1.000000     1.0    1058.5   0.643188   0.000251  0.507436

----------------------------------------


Validation score decreased (-0.621134 --> -0.621343).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#015.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#015.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#010.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#010.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430
9         0.616066        1.0     1.0     612.0   0.394421        0.0  0.352723
10        0.621134        1.0     1.0     680.0   0.400180        0.0  0.349939
11        0.576212        1.0     1.0     748.0   0.401411        0.0  0.342579
12        0.577153        1.0     1.0     816.0   0.392007        0.0  0.326218
13        0.569557        1.0     1.0     889.0   0.366494        0.0  0.294607
14        0.588229        1.0     1.0     957.0   0.381090        0.0  0.298231
15        0.621343        1.0     1.0    1025.0   0.403029        0.0  0.307289
16        0.587273        1.0     1.0    1093.0   0.387381        0.0  0.284495

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152
9         0.570284   1.000000     1.0     645.5   0.661025   0.000693  0.592675
10        0.582725   1.000000     1.0     713.5   0.657191   0.000488  0.578160
11        0.563951   1.000000     1.0     781.5   0.656484   0.000457  0.566448
12        0.578383   1.000000     1.0     852.0   0.672178   0.000457  0.568507
13        0.582134   1.000000     1.0     922.5   0.659761   0.000374  0.545384
14        0.578383   1.000000     1.0     990.5   0.649775   0.000271  0.524897
15        0.584468   1.000000     1.0    1058.5   0.643188   0.000251  0.507436
16        0.575996   1.000000     1.0    1126.5   0.647022   0.000248  0.498718

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430
9         0.616066        1.0     1.0     612.0   0.394421        0.0  0.352723
10        0.621134        1.0     1.0     680.0   0.400180        0.0  0.349939
11        0.576212        1.0     1.0     748.0   0.401411        0.0  0.342579
12        0.577153        1.0     1.0     816.0   0.392007        0.0  0.326218
13        0.569557        1.0     1.0     889.0   0.366494        0.0  0.294607
14        0.588229        1.0     1.0     957.0   0.381090        0.0  0.298231
15        0.621343        1.0     1.0    1025.0   0.403029        0.0  0.307289
16        0.587273        1.0     1.0    1093.0   0.387381        0.0  0.284495
17        0.598933        1.0     1.0    1161.0   0.397847        0.0  0.282834

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152
9         0.570284   1.000000     1.0     645.5   0.661025   0.000693  0.592675
10        0.582725   1.000000     1.0     713.5   0.657191   0.000488  0.578160
11        0.563951   1.000000     1.0     781.5   0.656484   0.000457  0.566448
12        0.578383   1.000000     1.0     852.0   0.672178   0.000457  0.568507
13        0.582134   1.000000     1.0     922.5   0.659761   0.000374  0.545384
14        0.578383   1.000000     1.0     990.5   0.649775   0.000271  0.524897
15        0.584468   1.000000     1.0    1058.5   0.643188   0.000251  0.507436
16        0.575996   1.000000     1.0    1126.5   0.647022   0.000248  0.498718
17        0.592722   1.000000     1.0    1194.5   0.639327   0.000165  0.479969

----------------------------------------


EarlyStopping counter: 2 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430
9         0.616066        1.0     1.0     612.0   0.394421        0.0  0.352723
10        0.621134        1.0     1.0     680.0   0.400180        0.0  0.349939
11        0.576212        1.0     1.0     748.0   0.401411        0.0  0.342579
12        0.577153        1.0     1.0     816.0   0.392007        0.0  0.326218
13        0.569557        1.0     1.0     889.0   0.366494        0.0  0.294607
14        0.588229        1.0     1.0     957.0   0.381090        0.0  0.298231
15        0.621343        1.0     1.0    1025.0   0.403029        0.0  0.307289
16        0.587273        1.0     1.0    1093.0   0.387381        0.0  0.284495
17        0.598933        1.0     1.0    1161.0   0.397847        0.0  0.282834
18        0.624155        1.0     1.0    1229.0   0.419246        0.0  0.289633

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152
9         0.570284   1.000000     1.0     645.5   0.661025   0.000693  0.592675
10        0.582725   1.000000     1.0     713.5   0.657191   0.000488  0.578160
11        0.563951   1.000000     1.0     781.5   0.656484   0.000457  0.566448
12        0.578383   1.000000     1.0     852.0   0.672178   0.000457  0.568507
13        0.582134   1.000000     1.0     922.5   0.659761   0.000374  0.545384
14        0.578383   1.000000     1.0     990.5   0.649775   0.000271  0.524897
15        0.584468   1.000000     1.0    1058.5   0.643188   0.000251  0.507436
16        0.575996   1.000000     1.0    1126.5   0.647022   0.000248  0.498718
17        0.592722   1.000000     1.0    1194.5   0.639327   0.000165  0.479969
18        0.605835   1.000000     1.0    1262.5   0.632243   0.000173  0.461791

----------------------------------------


Validation score decreased (-0.621343 --> -0.624155).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#018.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#018.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#1_epoch#015.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#1_epoch#015.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521748        0.0     1.0       0.0   0.348560        0.0  0.348560
1         0.558593        1.0     1.0      68.0   0.365551        0.0  0.368343
2         0.521326        1.0     1.0     136.0   0.391294        0.0  0.392280
3         0.515293        1.0     1.0     204.0   0.390159        0.0  0.387012
4         0.555555        1.0     1.0     272.0   0.396822        0.0  0.388083
5         0.538521        1.0     1.0     340.0   0.396241        0.0  0.381497
6         0.573515        1.0     1.0     408.0   0.397286        0.0  0.376572
7         0.540169        1.0     1.0     476.0   0.399276        0.0  0.371536
8         0.600767        1.0     1.0     544.0   0.400578        0.0  0.365430
9         0.616066        1.0     1.0     612.0   0.394421        0.0  0.352723
10        0.621134        1.0     1.0     680.0   0.400180        0.0  0.349939
11        0.576212        1.0     1.0     748.0   0.401411        0.0  0.342579
12        0.577153        1.0     1.0     816.0   0.392007        0.0  0.326218
13        0.569557        1.0     1.0     889.0   0.366494        0.0  0.294607
14        0.588229        1.0     1.0     957.0   0.381090        0.0  0.298231
15        0.621343        1.0     1.0    1025.0   0.403029        0.0  0.307289
16        0.587273        1.0     1.0    1093.0   0.387381        0.0  0.284495
17        0.598933        1.0     1.0    1161.0   0.397847        0.0  0.282834
18        0.624155        1.0     1.0    1229.0   0.419246        0.0  0.289633
19        0.577599        1.0     1.0    1297.0   0.392929        0.0  0.257137

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.496069   0.375000     1.0      33.5   0.737612   0.832340  1.568721
1         0.513978   0.970588     1.0     101.5   0.707747   0.145765  0.852887
2         0.517247   1.000000     1.0     169.5   0.696829   0.021958  0.713250
3         0.547589   1.000000     1.0     237.5   0.681936   0.007150  0.676366
4         0.547064   1.000000     1.0     305.5   0.675840   0.004087  0.659088
5         0.556267   1.000000     1.0     373.5   0.673765   0.002406  0.646468
6         0.572130   1.000000     1.0     441.5   0.668983   0.001455  0.631522
7         0.571836   1.000000     1.0     509.5   0.669281   0.001085  0.621530
8         0.562489   1.000000     1.0     577.5   0.665189   0.000809  0.607152
9         0.570284   1.000000     1.0     645.5   0.661025   0.000693  0.592675
10        0.582725   1.000000     1.0     713.5   0.657191   0.000488  0.578160
11        0.563951   1.000000     1.0     781.5   0.656484   0.000457  0.566448
12        0.578383   1.000000     1.0     852.0   0.672178   0.000457  0.568507
13        0.582134   1.000000     1.0     922.5   0.659761   0.000374  0.545384
14        0.578383   1.000000     1.0     990.5   0.649775   0.000271  0.524897
15        0.584468   1.000000     1.0    1058.5   0.643188   0.000251  0.507436
16        0.575996   1.000000     1.0    1126.5   0.647022   0.000248  0.498718
17        0.592722   1.000000     1.0    1194.5   0.639327   0.000165  0.479969
18        0.605835   1.000000     1.0    1262.5   0.632243   0.000173  0.461791
19        0.600342   1.000000     1.0    1330.5   0.637913   0.000141  0.453987

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Test: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
18        0.556968        1.0     1.0    1297.0   0.689291        0.0  0.502844

----------------------------------------


Matplotilb has been configured as follows:
{'figure.dpi': 100, 'savefig.dpi': 300, 'figure.figsize': '(7.0, 7.0) [cm]', 'font.size': 7, 'axes.labelsize': 8, 'xtick.labelsize': 8, 'ytick.labelsize': 8, 'axes.titlesize': 8, 'legend.fontsize': 6, 'pdf.fonttype': 42, 'ps.fonttype': 42, 'axes.spines.top': True, 'axes.spines.right': True, 'xtick.major.size': 2.032, 'ytick.major.size': 2.032, 'xtick.major.width': 0.508, 'ytick.major.width': 0.508}.


Balanced ACC in fold#1 was 0.522


MCC in fold#1 was 0.067


Confusion Matrix in fold#1: 

            NoN-Ripple  Ripple
NoN-Ripple        2478     287
Ripple            1624     283


Classification Report for fold#1:

             NoN-Ripple    Ripple  balanced accuracy  macro avg  weighted avg
precision         0.604     0.496              0.522      0.550         0.560
recall            0.896     0.148              0.522      0.522         0.591
f1-score          0.722     0.229              0.522      0.475         0.520
sample size    2765.000  1907.000              0.522   4672.000      4672.000


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/fold#1/NoN-Ripple.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/fold#1/Ripple.csv


Matplotilb has been configured as follows:
{'figure.dpi': 300, 'savefig.dpi': 300, 'figure.figsize': '(32.4, 20.0) [cm]', 'font.size': 16, 'axes.labelsize': 16, 'xtick.labelsize': 16, 'ytick.labelsize': 16, 'axes.titlesize': 16, 'legend.fontsize': 'xx-small', 'pdf.fonttype': 42, 'ps.fonttype': 42, 'axes.spines.top': True, 'axes.spines.right': True}.


 ---------------------------------------- fold#2 starts. ---------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Conv2d: 1-1                            [-1, 40, 1, 97]           1,000
├─Mish: 1-2                              [-1, 40, 1, 97]           --
├─Conv2d: 1-3                            [-1, 40, 1, 94]           6,440
├─BatchNorm2d: 1-4                       [-1, 40, 1, 94]           80
├─MaxPool2d: 1-5                         [-1, 40, 1, 18]           --
├─Mish: 1-6                              [-1, 40, 1, 18]           --
├─SwapLayer: 1-7                         [-1, 1, 40, 18]           --
├─Conv2d: 1-8                            [-1, 50, 33, 7]           4,850
├─BatchNorm2d: 1-9                       [-1, 50, 33, 7]           100
├─MaxPool2d: 1-10                        [-1, 50, 11, 2]           --
├─Mish: 1-11                             [-1, 50, 11, 2]           --
├─Conv2d: 1-12                           [-1, 50, 9, 2]            7,550
├─BatchNorm2d: 1-13                      [-1, 50, 9, 2]            100
├─MaxPool2d: 1-14                        [-1, 50, 9, 1]            --
├─Mish: 1-15                             [-1, 50, 9, 1]            --
├─Sequential: 1-16                       [-1, 2]                   --
|    └─Linear: 2-1                       [-1, 1024]                461,824
|    └─Mish: 2-2                         [-1, 1024]                --
|    └─Dropout: 2-3                      [-1, 1024]                --
|    └─Linear: 2-4                       [-1, 256]                 262,400
|    └─Mish: 2-5                         [-1, 256]                 --
|    └─Dropout: 2-6                      [-1, 256]                 --
|    └─Linear: 2-7                       [-1, 2]                   514
├─Sequential: 1-17                       [-1, 3]                   --
|    └─Linear: 2-8                       [-1, 1024]                461,824
|    └─Mish: 2-9                         [-1, 1024]                --
|    └─Dropout: 2-10                     [-1, 1024]                --
|    └─Linear: 2-11                      [-1, 256]                 262,400
|    └─Mish: 2-12                        [-1, 256]                 --
|    └─Dropout: 2-13                     [-1, 256]                 --
|    └─Linear: 2-14                      [-1, 3]                   771
==========================================================================================
Total params: 1,469,853
Trainable params: 1,469,853
Non-trainable params: 0
Total mult-adds (M): 4.83
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 0.30
Params size (MB): 5.61
Estimated Total Size (MB): 6.05
==========================================================================================

----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512

----------------------------------------

==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Conv2d: 1-1                            [-1, 40, 1, 97]           1,000
├─Mish: 1-2                              [-1, 40, 1, 97]           --
├─Conv2d: 1-3                            [-1, 40, 1, 94]           6,440
├─BatchNorm2d: 1-4                       [-1, 40, 1, 94]           80
├─MaxPool2d: 1-5                         [-1, 40, 1, 18]           --
├─Mish: 1-6                              [-1, 40, 1, 18]           --
├─SwapLayer: 1-7                         [-1, 1, 40, 18]           --
├─Conv2d: 1-8                            [-1, 50, 33, 7]           4,850
├─BatchNorm2d: 1-9                       [-1, 50, 33, 7]           100
├─MaxPool2d: 1-10                        [-1, 50, 11, 2]           --
├─Mish: 1-11                             [-1, 50, 11, 2]           --
├─Conv2d: 1-12                           [-1, 50, 9, 2]            7,550
├─BatchNorm2d: 1-13                      [-1, 50, 9, 2]            100
├─MaxPool2d: 1-14                        [-1, 50, 9, 1]            --
├─Mish: 1-15                             [-1, 50, 9, 1]            --
├─Sequential: 1-16                       [-1, 2]                   --
|    └─Linear: 2-1                       [-1, 1024]                461,824
|    └─Mish: 2-2                         [-1, 1024]                --
|    └─Dropout: 2-3                      [-1, 1024]                --
|    └─Linear: 2-4                       [-1, 256]                 262,400
|    └─Mish: 2-5                         [-1, 256]                 --
|    └─Dropout: 2-6                      [-1, 256]                 --
|    └─Linear: 2-7                       [-1, 2]                   514
├─Sequential: 1-17                       [-1, 3]                   --
|    └─Linear: 2-8                       [-1, 1024]                461,824
|    └─Mish: 2-9                         [-1, 1024]                --
|    └─Dropout: 2-10                     [-1, 1024]                --
|    └─Linear: 2-11                      [-1, 256]                 262,400
|    └─Mish: 2-12                        [-1, 256]                 --
|    └─Dropout: 2-13                     [-1, 256]                 --
|    └─Linear: 2-14                      [-1, 3]                   771
==========================================================================================
Total params: 1,469,853
Trainable params: 1,469,853
Non-trainable params: 0
Total mult-adds (M): 4.83
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 0.30
Params size (MB): 5.61
Estimated Total Size (MB): 6.05
==========================================================================================

----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782

----------------------------------------


Validation score decreased (inf --> -0.521959).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#2_epoch#000.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#2_epoch#000.pth

remove: path should be string, bytes or os.PathLike, not NoneType

----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908

----------------------------------------


Validation score decreased (-0.521959 --> -0.598308).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#2_epoch#001.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#2_epoch#001.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#2_epoch#000.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#2_epoch#000.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285

----------------------------------------


Validation score decreased (-0.598308 --> -0.599086).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#2_epoch#002.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#2_epoch#002.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#2_epoch#001.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#2_epoch#001.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959

----------------------------------------


EarlyStopping counter: 2 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623

----------------------------------------


Validation score decreased (-0.599086 --> -0.621772).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#2_epoch#005.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#2_epoch#005.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#2_epoch#002.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#2_epoch#002.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124

----------------------------------------


EarlyStopping counter: 2 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811

----------------------------------------


EarlyStopping counter: 3 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089
9         0.580916        1.0     2.0     612.0   0.393578        0.0  0.350762

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811
9         0.552131   1.000000     2.0     645.5   0.662596   0.000607  0.592442

----------------------------------------


EarlyStopping counter: 4 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089
9         0.580916        1.0     2.0     612.0   0.393578        0.0  0.350762
10        0.576073        1.0     2.0     680.0   0.392006        0.0  0.341296

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811
9         0.552131   1.000000     2.0     645.5   0.662596   0.000607  0.592442
10        0.563800   1.000000     2.0     713.5   0.667103   0.000443  0.585395

----------------------------------------


EarlyStopping counter: 5 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089
9         0.580916        1.0     2.0     612.0   0.393578        0.0  0.350762
10        0.576073        1.0     2.0     680.0   0.392006        0.0  0.341296
11        0.563021        1.0     2.0     748.0   0.378082        0.0  0.320634

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811
9         0.552131   1.000000     2.0     645.5   0.662596   0.000607  0.592442
10        0.563800   1.000000     2.0     713.5   0.667103   0.000443  0.585395
11        0.580953   1.000000     2.0     781.5   0.652670   0.000385  0.561269

----------------------------------------


EarlyStopping counter: 6 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089
9         0.580916        1.0     2.0     612.0   0.393578        0.0  0.350762
10        0.576073        1.0     2.0     680.0   0.392006        0.0  0.341296
11        0.563021        1.0     2.0     748.0   0.378082        0.0  0.320634
12        0.557309        1.0     2.0     816.0   0.394787        0.0  0.327588

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811
9         0.552131   1.000000     2.0     645.5   0.662596   0.000607  0.592442
10        0.563800   1.000000     2.0     713.5   0.667103   0.000443  0.585395
11        0.580953   1.000000     2.0     781.5   0.652670   0.000385  0.561269
12        0.580764   1.000000     2.0     849.5   0.648792   0.000433  0.546699

----------------------------------------


EarlyStopping counter: 7 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089
9         0.580916        1.0     2.0     612.0   0.393578        0.0  0.350762
10        0.576073        1.0     2.0     680.0   0.392006        0.0  0.341296
11        0.563021        1.0     2.0     748.0   0.378082        0.0  0.320634
12        0.557309        1.0     2.0     816.0   0.394787        0.0  0.327588
13        0.560300        1.0     2.0     884.0   0.381869        0.0  0.307102

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811
9         0.552131   1.000000     2.0     645.5   0.662596   0.000607  0.592442
10        0.563800   1.000000     2.0     713.5   0.667103   0.000443  0.585395
11        0.580953   1.000000     2.0     781.5   0.652670   0.000385  0.561269
12        0.580764   1.000000     2.0     849.5   0.648792   0.000433  0.546699
13        0.566439   1.000000     2.0     917.5   0.654434   0.000386  0.540123

----------------------------------------


EarlyStopping counter: 8 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089
9         0.580916        1.0     2.0     612.0   0.393578        0.0  0.350762
10        0.576073        1.0     2.0     680.0   0.392006        0.0  0.341296
11        0.563021        1.0     2.0     748.0   0.378082        0.0  0.320634
12        0.557309        1.0     2.0     816.0   0.394787        0.0  0.327588
13        0.560300        1.0     2.0     884.0   0.381869        0.0  0.307102
14        0.563694        1.0     2.0     952.0   0.397307        0.0  0.311157

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811
9         0.552131   1.000000     2.0     645.5   0.662596   0.000607  0.592442
10        0.563800   1.000000     2.0     713.5   0.667103   0.000443  0.585395
11        0.580953   1.000000     2.0     781.5   0.652670   0.000385  0.561269
12        0.580764   1.000000     2.0     849.5   0.648792   0.000433  0.546699
13        0.566439   1.000000     2.0     917.5   0.654434   0.000386  0.540123
14        0.557071   1.000000     2.0     985.5   0.652389   0.000301  0.526523

----------------------------------------


EarlyStopping counter: 9 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089
9         0.580916        1.0     2.0     612.0   0.393578        0.0  0.350762
10        0.576073        1.0     2.0     680.0   0.392006        0.0  0.341296
11        0.563021        1.0     2.0     748.0   0.378082        0.0  0.320634
12        0.557309        1.0     2.0     816.0   0.394787        0.0  0.327588
13        0.560300        1.0     2.0     884.0   0.381869        0.0  0.307102
14        0.563694        1.0     2.0     952.0   0.397307        0.0  0.311157
15        0.567241        1.0     2.0    1020.0   0.385642        0.0  0.292243

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811
9         0.552131   1.000000     2.0     645.5   0.662596   0.000607  0.592442
10        0.563800   1.000000     2.0     713.5   0.667103   0.000443  0.585395
11        0.580953   1.000000     2.0     781.5   0.652670   0.000385  0.561269
12        0.580764   1.000000     2.0     849.5   0.648792   0.000433  0.546699
13        0.566439   1.000000     2.0     917.5   0.654434   0.000386  0.540123
14        0.557071   1.000000     2.0     985.5   0.652389   0.000301  0.526523
15        0.597789   1.000000     2.0    1053.5   0.640479   0.000237  0.504443

----------------------------------------


EarlyStopping counter: 10 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089
9         0.580916        1.0     2.0     612.0   0.393578        0.0  0.350762
10        0.576073        1.0     2.0     680.0   0.392006        0.0  0.341296
11        0.563021        1.0     2.0     748.0   0.378082        0.0  0.320634
12        0.557309        1.0     2.0     816.0   0.394787        0.0  0.327588
13        0.560300        1.0     2.0     884.0   0.381869        0.0  0.307102
14        0.563694        1.0     2.0     952.0   0.397307        0.0  0.311157
15        0.567241        1.0     2.0    1020.0   0.385642        0.0  0.292243
16        0.577801        1.0     2.0    1088.0   0.396239        0.0  0.291012

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811
9         0.552131   1.000000     2.0     645.5   0.662596   0.000607  0.592442
10        0.563800   1.000000     2.0     713.5   0.667103   0.000443  0.585395
11        0.580953   1.000000     2.0     781.5   0.652670   0.000385  0.561269
12        0.580764   1.000000     2.0     849.5   0.648792   0.000433  0.546699
13        0.566439   1.000000     2.0     917.5   0.654434   0.000386  0.540123
14        0.557071   1.000000     2.0     985.5   0.652389   0.000301  0.526523
15        0.597789   1.000000     2.0    1053.5   0.640479   0.000237  0.504443
16        0.581579   1.000000     2.0    1121.5   0.640066   0.000304  0.492238

----------------------------------------


EarlyStopping counter: 11 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089
9         0.580916        1.0     2.0     612.0   0.393578        0.0  0.350762
10        0.576073        1.0     2.0     680.0   0.392006        0.0  0.341296
11        0.563021        1.0     2.0     748.0   0.378082        0.0  0.320634
12        0.557309        1.0     2.0     816.0   0.394787        0.0  0.327588
13        0.560300        1.0     2.0     884.0   0.381869        0.0  0.307102
14        0.563694        1.0     2.0     952.0   0.397307        0.0  0.311157
15        0.567241        1.0     2.0    1020.0   0.385642        0.0  0.292243
16        0.577801        1.0     2.0    1088.0   0.396239        0.0  0.291012
17        0.548837        1.0     2.0    1156.0   0.379823        0.0  0.266500

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811
9         0.552131   1.000000     2.0     645.5   0.662596   0.000607  0.592442
10        0.563800   1.000000     2.0     713.5   0.667103   0.000443  0.585395
11        0.580953   1.000000     2.0     781.5   0.652670   0.000385  0.561269
12        0.580764   1.000000     2.0     849.5   0.648792   0.000433  0.546699
13        0.566439   1.000000     2.0     917.5   0.654434   0.000386  0.540123
14        0.557071   1.000000     2.0     985.5   0.652389   0.000301  0.526523
15        0.597789   1.000000     2.0    1053.5   0.640479   0.000237  0.504443
16        0.581579   1.000000     2.0    1121.5   0.640066   0.000304  0.492238
17        0.594496   1.000000     2.0    1189.5   0.642263   0.000216  0.481821

----------------------------------------


EarlyStopping counter: 12 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089
9         0.580916        1.0     2.0     612.0   0.393578        0.0  0.350762
10        0.576073        1.0     2.0     680.0   0.392006        0.0  0.341296
11        0.563021        1.0     2.0     748.0   0.378082        0.0  0.320634
12        0.557309        1.0     2.0     816.0   0.394787        0.0  0.327588
13        0.560300        1.0     2.0     884.0   0.381869        0.0  0.307102
14        0.563694        1.0     2.0     952.0   0.397307        0.0  0.311157
15        0.567241        1.0     2.0    1020.0   0.385642        0.0  0.292243
16        0.577801        1.0     2.0    1088.0   0.396239        0.0  0.291012
17        0.548837        1.0     2.0    1156.0   0.379823        0.0  0.266500
18        0.556935        1.0     2.0    1224.0   0.372560        0.0  0.250579

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811
9         0.552131   1.000000     2.0     645.5   0.662596   0.000607  0.592442
10        0.563800   1.000000     2.0     713.5   0.667103   0.000443  0.585395
11        0.580953   1.000000     2.0     781.5   0.652670   0.000385  0.561269
12        0.580764   1.000000     2.0     849.5   0.648792   0.000433  0.546699
13        0.566439   1.000000     2.0     917.5   0.654434   0.000386  0.540123
14        0.557071   1.000000     2.0     985.5   0.652389   0.000301  0.526523
15        0.597789   1.000000     2.0    1053.5   0.640479   0.000237  0.504443
16        0.581579   1.000000     2.0    1121.5   0.640066   0.000304  0.492238
17        0.594496   1.000000     2.0    1189.5   0.642263   0.000216  0.481821
18        0.592963   1.000000     2.0    1257.5   0.634079   0.000243  0.462726

----------------------------------------


EarlyStopping counter: 13 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521959        0.0     2.0       0.0   0.344512        0.0  0.344512
1         0.598308        1.0     2.0      68.0   0.362139        0.0  0.364719
2         0.599086        1.0     2.0     136.0   0.385856        0.0  0.386194
3         0.596326        1.0     2.0     204.0   0.381518        0.0  0.377638
4         0.598578        1.0     2.0     272.0   0.384964        0.0  0.375581
5         0.621772        1.0     2.0     340.0   0.388792        0.0  0.373264
6         0.587784        1.0     2.0     408.0   0.379823        0.0  0.358904
7         0.614796        1.0     2.0     476.0   0.398733        0.0  0.369901
8         0.613468        1.0     2.0     544.0   0.400404        0.0  0.364089
9         0.580916        1.0     2.0     612.0   0.393578        0.0  0.350762
10        0.576073        1.0     2.0     680.0   0.392006        0.0  0.341296
11        0.563021        1.0     2.0     748.0   0.378082        0.0  0.320634
12        0.557309        1.0     2.0     816.0   0.394787        0.0  0.327588
13        0.560300        1.0     2.0     884.0   0.381869        0.0  0.307102
14        0.563694        1.0     2.0     952.0   0.397307        0.0  0.311157
15        0.567241        1.0     2.0    1020.0   0.385642        0.0  0.292243
16        0.577801        1.0     2.0    1088.0   0.396239        0.0  0.291012
17        0.548837        1.0     2.0    1156.0   0.379823        0.0  0.266500
18        0.556935        1.0     2.0    1224.0   0.372560        0.0  0.250579
19        0.570754        1.0     2.0    1292.0   0.388659        0.0  0.252637

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.507105   0.389706     2.0      33.5   0.717358   0.764353  1.480782
1         0.521903   0.977941     2.0     101.5   0.692625   0.126114  0.817908
2         0.523689   1.000000     2.0     169.5   0.694968   0.015727  0.704285
3         0.531994   1.000000     2.0     237.5   0.683132   0.005299  0.674499
4         0.556931   1.000000     2.0     305.5   0.671026   0.002956  0.651959
5         0.549176   1.000000     2.0     373.5   0.674882   0.001929  0.645623
6         0.559629   1.000000     2.0     441.5   0.664963   0.001470  0.626245
7         0.549964   1.000000     2.0     509.5   0.664350   0.000836  0.615124
8         0.575932   1.000000     2.0     577.5   0.659824   0.000931  0.600811
9         0.552131   1.000000     2.0     645.5   0.662596   0.000607  0.592442
10        0.563800   1.000000     2.0     713.5   0.667103   0.000443  0.585395
11        0.580953   1.000000     2.0     781.5   0.652670   0.000385  0.561269
12        0.580764   1.000000     2.0     849.5   0.648792   0.000433  0.546699
13        0.566439   1.000000     2.0     917.5   0.654434   0.000386  0.540123
14        0.557071   1.000000     2.0     985.5   0.652389   0.000301  0.526523
15        0.597789   1.000000     2.0    1053.5   0.640479   0.000237  0.504443
16        0.581579   1.000000     2.0    1121.5   0.640066   0.000304  0.492238
17        0.594496   1.000000     2.0    1189.5   0.642263   0.000216  0.481821
18        0.592963   1.000000     2.0    1257.5   0.634079   0.000243  0.462726
19        0.587539   1.000000     2.0    1325.5   0.638713   0.000202  0.454021

----------------------------------------


EarlyStopping counter: 14 out of 50


----------------------------------------


Test: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
5         0.559891        1.0     2.0     408.0   0.689364        0.0  0.652905

----------------------------------------


Matplotilb has been configured as follows:
{'figure.dpi': 100, 'savefig.dpi': 300, 'figure.figsize': '(7.0, 7.0) [cm]', 'font.size': 7, 'axes.labelsize': 8, 'xtick.labelsize': 8, 'ytick.labelsize': 8, 'axes.titlesize': 8, 'legend.fontsize': 6, 'pdf.fonttype': 42, 'ps.fonttype': 42, 'axes.spines.top': True, 'axes.spines.right': True, 'xtick.major.size': 2.032, 'ytick.major.size': 2.032, 'xtick.major.width': 0.508, 'ytick.major.width': 0.508}.


Balanced ACC in fold#2 was 0.518


MCC in fold#2 was 0.074


Confusion Matrix in fold#2: 

            NoN-Ripple  Ripple
NoN-Ripple        2635     130
Ripple            1748     159


Classification Report for fold#2:

             NoN-Ripple    Ripple  balanced accuracy  macro avg  weighted avg
precision         0.601     0.550              0.518      0.576         0.580
recall            0.953     0.083              0.518      0.518         0.598
f1-score          0.737     0.145              0.518      0.441         0.495
sample size    2765.000  1907.000              0.518   4672.000      4672.000


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/fold#2/NoN-Ripple.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/fold#2/Ripple.csv


Matplotilb has been configured as follows:
{'figure.dpi': 300, 'savefig.dpi': 300, 'figure.figsize': '(32.4, 20.0) [cm]', 'font.size': 16, 'axes.labelsize': 16, 'xtick.labelsize': 16, 'ytick.labelsize': 16, 'axes.titlesize': 16, 'legend.fontsize': 'xx-small', 'pdf.fonttype': 42, 'ps.fonttype': 42, 'axes.spines.top': True, 'axes.spines.right': True}.


 ---------------------------------------- fold#3 starts. ---------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Conv2d: 1-1                            [-1, 40, 1, 97]           1,000
├─Mish: 1-2                              [-1, 40, 1, 97]           --
├─Conv2d: 1-3                            [-1, 40, 1, 94]           6,440
├─BatchNorm2d: 1-4                       [-1, 40, 1, 94]           80
├─MaxPool2d: 1-5                         [-1, 40, 1, 18]           --
├─Mish: 1-6                              [-1, 40, 1, 18]           --
├─SwapLayer: 1-7                         [-1, 1, 40, 18]           --
├─Conv2d: 1-8                            [-1, 50, 33, 7]           4,850
├─BatchNorm2d: 1-9                       [-1, 50, 33, 7]           100
├─MaxPool2d: 1-10                        [-1, 50, 11, 2]           --
├─Mish: 1-11                             [-1, 50, 11, 2]           --
├─Conv2d: 1-12                           [-1, 50, 9, 2]            7,550
├─BatchNorm2d: 1-13                      [-1, 50, 9, 2]            100
├─MaxPool2d: 1-14                        [-1, 50, 9, 1]            --
├─Mish: 1-15                             [-1, 50, 9, 1]            --
├─Sequential: 1-16                       [-1, 2]                   --
|    └─Linear: 2-1                       [-1, 1024]                461,824
|    └─Mish: 2-2                         [-1, 1024]                --
|    └─Dropout: 2-3                      [-1, 1024]                --
|    └─Linear: 2-4                       [-1, 256]                 262,400
|    └─Mish: 2-5                         [-1, 256]                 --
|    └─Dropout: 2-6                      [-1, 256]                 --
|    └─Linear: 2-7                       [-1, 2]                   514
├─Sequential: 1-17                       [-1, 3]                   --
|    └─Linear: 2-8                       [-1, 1024]                461,824
|    └─Mish: 2-9                         [-1, 1024]                --
|    └─Dropout: 2-10                     [-1, 1024]                --
|    └─Linear: 2-11                      [-1, 256]                 262,400
|    └─Mish: 2-12                        [-1, 256]                 --
|    └─Dropout: 2-13                     [-1, 256]                 --
|    └─Linear: 2-14                      [-1, 3]                   771
==========================================================================================
Total params: 1,469,853
Trainable params: 1,469,853
Non-trainable params: 0
Total mult-adds (M): 4.83
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 0.30
Params size (MB): 5.61
Estimated Total Size (MB): 6.05
==========================================================================================

----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0          0.52027        0.0     3.0       0.0   0.344457        0.0  0.344457

----------------------------------------

==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Conv2d: 1-1                            [-1, 40, 1, 97]           1,000
├─Mish: 1-2                              [-1, 40, 1, 97]           --
├─Conv2d: 1-3                            [-1, 40, 1, 94]           6,440
├─BatchNorm2d: 1-4                       [-1, 40, 1, 94]           80
├─MaxPool2d: 1-5                         [-1, 40, 1, 18]           --
├─Mish: 1-6                              [-1, 40, 1, 18]           --
├─SwapLayer: 1-7                         [-1, 1, 40, 18]           --
├─Conv2d: 1-8                            [-1, 50, 33, 7]           4,850
├─BatchNorm2d: 1-9                       [-1, 50, 33, 7]           100
├─MaxPool2d: 1-10                        [-1, 50, 11, 2]           --
├─Mish: 1-11                             [-1, 50, 11, 2]           --
├─Conv2d: 1-12                           [-1, 50, 9, 2]            7,550
├─BatchNorm2d: 1-13                      [-1, 50, 9, 2]            100
├─MaxPool2d: 1-14                        [-1, 50, 9, 1]            --
├─Mish: 1-15                             [-1, 50, 9, 1]            --
├─Sequential: 1-16                       [-1, 2]                   --
|    └─Linear: 2-1                       [-1, 1024]                461,824
|    └─Mish: 2-2                         [-1, 1024]                --
|    └─Dropout: 2-3                      [-1, 1024]                --
|    └─Linear: 2-4                       [-1, 256]                 262,400
|    └─Mish: 2-5                         [-1, 256]                 --
|    └─Dropout: 2-6                      [-1, 256]                 --
|    └─Linear: 2-7                       [-1, 2]                   514
├─Sequential: 1-17                       [-1, 3]                   --
|    └─Linear: 2-8                       [-1, 1024]                461,824
|    └─Mish: 2-9                         [-1, 1024]                --
|    └─Dropout: 2-10                     [-1, 1024]                --
|    └─Linear: 2-11                      [-1, 256]                 262,400
|    └─Mish: 2-12                        [-1, 256]                 --
|    └─Dropout: 2-13                     [-1, 256]                 --
|    └─Linear: 2-14                      [-1, 3]                   771
==========================================================================================
Total params: 1,469,853
Trainable params: 1,469,853
Non-trainable params: 0
Total mult-adds (M): 4.83
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 0.30
Params size (MB): 5.61
Estimated Total Size (MB): 6.05
==========================================================================================

----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895

----------------------------------------


Validation score decreased (inf --> -0.520270).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#3_epoch#000.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#3_epoch#000.pth

remove: path should be string, bytes or os.PathLike, not NoneType

----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790

----------------------------------------


Validation score decreased (-0.520270 --> -0.521748).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#3_epoch#001.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#3_epoch#001.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#3_epoch#000.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#3_epoch#000.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424

----------------------------------------


Validation score decreased (-0.521748 --> -0.521748).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#3_epoch#002.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#3_epoch#002.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#3_epoch#001.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#3_epoch#001.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512

----------------------------------------


Validation score decreased (-0.521748 --> -0.590160).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#3_epoch#003.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#3_epoch#003.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#3_epoch#002.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#3_epoch#002.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740

----------------------------------------


EarlyStopping counter: 2 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031

----------------------------------------


EarlyStopping counter: 3 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264

----------------------------------------


Validation score decreased (-0.590160 --> -0.596270).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#3_epoch#007.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#3_epoch#007.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#3_epoch#003.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#3_epoch#003.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653

----------------------------------------


Validation score decreased (-0.596270 --> -0.624380).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#3_epoch#008.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#3_epoch#008.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#3_epoch#007.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#3_epoch#007.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285
9         0.606782        1.0     3.0     612.0   0.395678        0.0  0.355563

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653
9         0.556716   1.000000     3.0     645.5   0.663508   0.000707  0.596088

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285
9         0.606782        1.0     3.0     612.0   0.395678        0.0  0.355563
10        0.610790        1.0     3.0     680.0   0.405133        0.0  0.356157

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653
9         0.556716   1.000000     3.0     645.5   0.663508   0.000707  0.596088
10        0.561350   1.000000     3.0     713.5   0.660429   0.000573  0.582398

----------------------------------------


EarlyStopping counter: 2 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285
9         0.606782        1.0     3.0     612.0   0.395678        0.0  0.355563
10        0.610790        1.0     3.0     680.0   0.405133        0.0  0.356157
11        0.597853        1.0     3.0     748.0   0.394898        0.0  0.338627

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653
9         0.556716   1.000000     3.0     645.5   0.663508   0.000707  0.596088
10        0.561350   1.000000     3.0     713.5   0.660429   0.000573  0.582398
11        0.560999   1.000000     3.0     781.5   0.657065   0.000444  0.568239

----------------------------------------


EarlyStopping counter: 3 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285
9         0.606782        1.0     3.0     612.0   0.395678        0.0  0.355563
10        0.610790        1.0     3.0     680.0   0.405133        0.0  0.356157
11        0.597853        1.0     3.0     748.0   0.394898        0.0  0.338627
12        0.611929        1.0     3.0     816.0   0.408862        0.0  0.343061

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653
9         0.556716   1.000000     3.0     645.5   0.663508   0.000707  0.596088
10        0.561350   1.000000     3.0     713.5   0.660429   0.000573  0.582398
11        0.560999   1.000000     3.0     781.5   0.657065   0.000444  0.568239
12        0.563677   1.000000     3.0     849.5   0.656345   0.000534  0.556510

----------------------------------------


EarlyStopping counter: 4 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285
9         0.606782        1.0     3.0     612.0   0.395678        0.0  0.355563
10        0.610790        1.0     3.0     680.0   0.405133        0.0  0.356157
11        0.597853        1.0     3.0     748.0   0.394898        0.0  0.338627
12        0.611929        1.0     3.0     816.0   0.408862        0.0  0.343061
13        0.584987        1.0     3.0     884.0   0.396123        0.0  0.322749

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653
9         0.556716   1.000000     3.0     645.5   0.663508   0.000707  0.596088
10        0.561350   1.000000     3.0     713.5   0.660429   0.000573  0.582398
11        0.560999   1.000000     3.0     781.5   0.657065   0.000444  0.568239
12        0.563677   1.000000     3.0     849.5   0.656345   0.000534  0.556510
13        0.580236   1.000000     3.0     917.5   0.646991   0.000322  0.536690

----------------------------------------


EarlyStopping counter: 5 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285
9         0.606782        1.0     3.0     612.0   0.395678        0.0  0.355563
10        0.610790        1.0     3.0     680.0   0.405133        0.0  0.356157
11        0.597853        1.0     3.0     748.0   0.394898        0.0  0.338627
12        0.611929        1.0     3.0     816.0   0.408862        0.0  0.343061
13        0.584987        1.0     3.0     884.0   0.396123        0.0  0.322749
14        0.621480        1.0     3.0     952.0   0.402212        0.0  0.318513

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653
9         0.556716   1.000000     3.0     645.5   0.663508   0.000707  0.596088
10        0.561350   1.000000     3.0     713.5   0.660429   0.000573  0.582398
11        0.560999   1.000000     3.0     781.5   0.657065   0.000444  0.568239
12        0.563677   1.000000     3.0     849.5   0.656345   0.000534  0.556510
13        0.580236   1.000000     3.0     917.5   0.646991   0.000322  0.536690
14        0.586736   1.000000     3.0     985.5   0.644962   0.000320  0.523399

----------------------------------------


EarlyStopping counter: 6 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285
9         0.606782        1.0     3.0     612.0   0.395678        0.0  0.355563
10        0.610790        1.0     3.0     680.0   0.405133        0.0  0.356157
11        0.597853        1.0     3.0     748.0   0.394898        0.0  0.338627
12        0.611929        1.0     3.0     816.0   0.408862        0.0  0.343061
13        0.584987        1.0     3.0     884.0   0.396123        0.0  0.322749
14        0.621480        1.0     3.0     952.0   0.402212        0.0  0.318513
15        0.585749        1.0     3.0    1020.0   0.387767        0.0  0.297010

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653
9         0.556716   1.000000     3.0     645.5   0.663508   0.000707  0.596088
10        0.561350   1.000000     3.0     713.5   0.660429   0.000573  0.582398
11        0.560999   1.000000     3.0     781.5   0.657065   0.000444  0.568239
12        0.563677   1.000000     3.0     849.5   0.656345   0.000534  0.556510
13        0.580236   1.000000     3.0     917.5   0.646991   0.000322  0.536690
14        0.586736   1.000000     3.0     985.5   0.644962   0.000320  0.523399
15        0.575519   1.000000     3.0    1053.5   0.648776   0.000252  0.514856

----------------------------------------


EarlyStopping counter: 7 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285
9         0.606782        1.0     3.0     612.0   0.395678        0.0  0.355563
10        0.610790        1.0     3.0     680.0   0.405133        0.0  0.356157
11        0.597853        1.0     3.0     748.0   0.394898        0.0  0.338627
12        0.611929        1.0     3.0     816.0   0.408862        0.0  0.343061
13        0.584987        1.0     3.0     884.0   0.396123        0.0  0.322749
14        0.621480        1.0     3.0     952.0   0.402212        0.0  0.318513
15        0.585749        1.0     3.0    1020.0   0.387767        0.0  0.297010
16        0.603541        1.0     3.0    1088.0   0.396232        0.0  0.294175

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653
9         0.556716   1.000000     3.0     645.5   0.663508   0.000707  0.596088
10        0.561350   1.000000     3.0     713.5   0.660429   0.000573  0.582398
11        0.560999   1.000000     3.0     781.5   0.657065   0.000444  0.568239
12        0.563677   1.000000     3.0     849.5   0.656345   0.000534  0.556510
13        0.580236   1.000000     3.0     917.5   0.646991   0.000322  0.536690
14        0.586736   1.000000     3.0     985.5   0.644962   0.000320  0.523399
15        0.575519   1.000000     3.0    1053.5   0.648776   0.000252  0.514856
16        0.581153   1.000000     3.0    1121.5   0.646766   0.000228  0.501126

----------------------------------------


EarlyStopping counter: 8 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285
9         0.606782        1.0     3.0     612.0   0.395678        0.0  0.355563
10        0.610790        1.0     3.0     680.0   0.405133        0.0  0.356157
11        0.597853        1.0     3.0     748.0   0.394898        0.0  0.338627
12        0.611929        1.0     3.0     816.0   0.408862        0.0  0.343061
13        0.584987        1.0     3.0     884.0   0.396123        0.0  0.322749
14        0.621480        1.0     3.0     952.0   0.402212        0.0  0.318513
15        0.585749        1.0     3.0    1020.0   0.387767        0.0  0.297010
16        0.603541        1.0     3.0    1088.0   0.396232        0.0  0.294175
17        0.573405        1.0     3.0    1156.0   0.404060        0.0  0.290325

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653
9         0.556716   1.000000     3.0     645.5   0.663508   0.000707  0.596088
10        0.561350   1.000000     3.0     713.5   0.660429   0.000573  0.582398
11        0.560999   1.000000     3.0     781.5   0.657065   0.000444  0.568239
12        0.563677   1.000000     3.0     849.5   0.656345   0.000534  0.556510
13        0.580236   1.000000     3.0     917.5   0.646991   0.000322  0.536690
14        0.586736   1.000000     3.0     985.5   0.644962   0.000320  0.523399
15        0.575519   1.000000     3.0    1053.5   0.648776   0.000252  0.514856
16        0.581153   1.000000     3.0    1121.5   0.646766   0.000228  0.501126
17        0.592421   1.000000     3.0    1189.5   0.638461   0.000228  0.482000

----------------------------------------


EarlyStopping counter: 9 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285
9         0.606782        1.0     3.0     612.0   0.395678        0.0  0.355563
10        0.610790        1.0     3.0     680.0   0.405133        0.0  0.356157
11        0.597853        1.0     3.0     748.0   0.394898        0.0  0.338627
12        0.611929        1.0     3.0     816.0   0.408862        0.0  0.343061
13        0.584987        1.0     3.0     884.0   0.396123        0.0  0.322749
14        0.621480        1.0     3.0     952.0   0.402212        0.0  0.318513
15        0.585749        1.0     3.0    1020.0   0.387767        0.0  0.297010
16        0.603541        1.0     3.0    1088.0   0.396232        0.0  0.294175
17        0.573405        1.0     3.0    1156.0   0.404060        0.0  0.290325
18        0.591468        1.0     3.0    1224.0   0.408768        0.0  0.284173

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653
9         0.556716   1.000000     3.0     645.5   0.663508   0.000707  0.596088
10        0.561350   1.000000     3.0     713.5   0.660429   0.000573  0.582398
11        0.560999   1.000000     3.0     781.5   0.657065   0.000444  0.568239
12        0.563677   1.000000     3.0     849.5   0.656345   0.000534  0.556510
13        0.580236   1.000000     3.0     917.5   0.646991   0.000322  0.536690
14        0.586736   1.000000     3.0     985.5   0.644962   0.000320  0.523399
15        0.575519   1.000000     3.0    1053.5   0.648776   0.000252  0.514856
16        0.581153   1.000000     3.0    1121.5   0.646766   0.000228  0.501126
17        0.592421   1.000000     3.0    1189.5   0.638461   0.000228  0.482000
18        0.590396   1.000000     3.0    1257.5   0.639592   0.000248  0.470723

----------------------------------------


EarlyStopping counter: 10 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.520270        0.0     3.0       0.0   0.344457        0.0  0.344457
1         0.521748        1.0     3.0      68.0   0.380322        0.0  0.383190
2         0.521748        1.0     3.0     136.0   0.401833        0.0  0.403182
3         0.590160        1.0     3.0     204.0   0.396941        0.0  0.394391
4         0.576984        1.0     3.0     272.0   0.406586        0.0  0.398484
5         0.578339        1.0     3.0     340.0   0.387048        0.0  0.373831
6         0.580210        1.0     3.0     408.0   0.392697        0.0  0.373586
7         0.596270        1.0     3.0     476.0   0.396388        0.0  0.370289
8         0.624380        1.0     3.0     544.0   0.398749        0.0  0.365285
9         0.606782        1.0     3.0     612.0   0.395678        0.0  0.355563
10        0.610790        1.0     3.0     680.0   0.405133        0.0  0.356157
11        0.597853        1.0     3.0     748.0   0.394898        0.0  0.338627
12        0.611929        1.0     3.0     816.0   0.408862        0.0  0.343061
13        0.584987        1.0     3.0     884.0   0.396123        0.0  0.322749
14        0.621480        1.0     3.0     952.0   0.402212        0.0  0.318513
15        0.585749        1.0     3.0    1020.0   0.387767        0.0  0.297010
16        0.603541        1.0     3.0    1088.0   0.396232        0.0  0.294175
17        0.573405        1.0     3.0    1156.0   0.404060        0.0  0.290325
18        0.591468        1.0     3.0    1224.0   0.408768        0.0  0.284173
19        0.560717        1.0     3.0    1292.0   0.398438        0.0  0.264247

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.515931   0.357843     3.0      33.5   0.717399   0.882898  1.598895
1         0.515194   0.953431     3.0     101.5   0.700480   0.156677  0.856790
2         0.519028   1.000000     3.0     169.5   0.700561   0.019924  0.715424
3         0.532416   1.000000     3.0     237.5   0.678384   0.007078  0.673512
4         0.532474   1.000000     3.0     305.5   0.688154   0.003714  0.671382
5         0.525826   1.000000     3.0     373.5   0.685056   0.002116  0.657740
6         0.560497   1.000000     3.0     441.5   0.668449   0.001620  0.632031
7         0.549653   1.000000     3.0     509.5   0.666683   0.001263  0.620264
8         0.560344   1.000000     3.0     577.5   0.669908   0.000870  0.612653
9         0.556716   1.000000     3.0     645.5   0.663508   0.000707  0.596088
10        0.561350   1.000000     3.0     713.5   0.660429   0.000573  0.582398
11        0.560999   1.000000     3.0     781.5   0.657065   0.000444  0.568239
12        0.563677   1.000000     3.0     849.5   0.656345   0.000534  0.556510
13        0.580236   1.000000     3.0     917.5   0.646991   0.000322  0.536690
14        0.586736   1.000000     3.0     985.5   0.644962   0.000320  0.523399
15        0.575519   1.000000     3.0    1053.5   0.648776   0.000252  0.514856
16        0.581153   1.000000     3.0    1121.5   0.646766   0.000228  0.501126
17        0.592421   1.000000     3.0    1189.5   0.638461   0.000228  0.482000
18        0.590396   1.000000     3.0    1257.5   0.639592   0.000248  0.470723
19        0.592861   1.000000     3.0    1325.5   0.638065   0.000199  0.456916

----------------------------------------


EarlyStopping counter: 11 out of 50


----------------------------------------


Test: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
8         0.549924        1.0     3.0     612.0   0.702194        0.0  0.636337

----------------------------------------


Matplotilb has been configured as follows:
{'figure.dpi': 100, 'savefig.dpi': 300, 'figure.figsize': '(7.0, 7.0) [cm]', 'font.size': 7, 'axes.labelsize': 8, 'xtick.labelsize': 8, 'ytick.labelsize': 8, 'axes.titlesize': 8, 'legend.fontsize': 6, 'pdf.fonttype': 42, 'ps.fonttype': 42, 'axes.spines.top': True, 'axes.spines.right': True, 'xtick.major.size': 2.032, 'ytick.major.size': 2.032, 'xtick.major.width': 0.508, 'ytick.major.width': 0.508}.


Balanced ACC in fold#3 was 0.519


MCC in fold#3 was 0.073


Confusion Matrix in fold#3: 

            NoN-Ripple  Ripple
NoN-Ripple        2603     162
Ripple            1721     186


Classification Report for fold#3:

             NoN-Ripple    Ripple  balanced accuracy  macro avg  weighted avg
precision         0.602     0.534              0.519      0.568         0.574
recall            0.941     0.098              0.519      0.519         0.597
f1-score          0.734     0.165              0.519      0.450         0.502
sample size    2765.000  1907.000              0.519   4672.000      4672.000


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/fold#3/NoN-Ripple.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/fold#3/Ripple.csv


Matplotilb has been configured as follows:
{'figure.dpi': 300, 'savefig.dpi': 300, 'figure.figsize': '(32.4, 20.0) [cm]', 'font.size': 16, 'axes.labelsize': 16, 'xtick.labelsize': 16, 'ytick.labelsize': 16, 'axes.titlesize': 16, 'legend.fontsize': 'xx-small', 'pdf.fonttype': 42, 'ps.fonttype': 42, 'axes.spines.top': True, 'axes.spines.right': True}.


 ---------------------------------------- fold#4 starts. ---------------------------------------- 

Ranger optimizer loaded. 
Gradient Centralization usage = True
GC applied to both conv and fc layers
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Conv2d: 1-1                            [-1, 40, 1, 97]           1,000
├─Mish: 1-2                              [-1, 40, 1, 97]           --
├─Conv2d: 1-3                            [-1, 40, 1, 94]           6,440
├─BatchNorm2d: 1-4                       [-1, 40, 1, 94]           80
├─MaxPool2d: 1-5                         [-1, 40, 1, 18]           --
├─Mish: 1-6                              [-1, 40, 1, 18]           --
├─SwapLayer: 1-7                         [-1, 1, 40, 18]           --
├─Conv2d: 1-8                            [-1, 50, 33, 7]           4,850
├─BatchNorm2d: 1-9                       [-1, 50, 33, 7]           100
├─MaxPool2d: 1-10                        [-1, 50, 11, 2]           --
├─Mish: 1-11                             [-1, 50, 11, 2]           --
├─Conv2d: 1-12                           [-1, 50, 9, 2]            7,550
├─BatchNorm2d: 1-13                      [-1, 50, 9, 2]            100
├─MaxPool2d: 1-14                        [-1, 50, 9, 1]            --
├─Mish: 1-15                             [-1, 50, 9, 1]            --
├─Sequential: 1-16                       [-1, 2]                   --
|    └─Linear: 2-1                       [-1, 1024]                461,824
|    └─Mish: 2-2                         [-1, 1024]                --
|    └─Dropout: 2-3                      [-1, 1024]                --
|    └─Linear: 2-4                       [-1, 256]                 262,400
|    └─Mish: 2-5                         [-1, 256]                 --
|    └─Dropout: 2-6                      [-1, 256]                 --
|    └─Linear: 2-7                       [-1, 2]                   514
├─Sequential: 1-17                       [-1, 3]                   --
|    └─Linear: 2-8                       [-1, 1024]                461,824
|    └─Mish: 2-9                         [-1, 1024]                --
|    └─Dropout: 2-10                     [-1, 1024]                --
|    └─Linear: 2-11                      [-1, 256]                 262,400
|    └─Mish: 2-12                        [-1, 256]                 --
|    └─Dropout: 2-13                     [-1, 256]                 --
|    └─Linear: 2-14                      [-1, 3]                   771
==========================================================================================
Total params: 1,469,853
Trainable params: 1,469,853
Non-trainable params: 0
Total mult-adds (M): 4.83
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 0.30
Params size (MB): 5.61
Estimated Total Size (MB): 6.05
==========================================================================================

----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583

----------------------------------------

==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Conv2d: 1-1                            [-1, 40, 1, 97]           1,000
├─Mish: 1-2                              [-1, 40, 1, 97]           --
├─Conv2d: 1-3                            [-1, 40, 1, 94]           6,440
├─BatchNorm2d: 1-4                       [-1, 40, 1, 94]           80
├─MaxPool2d: 1-5                         [-1, 40, 1, 18]           --
├─Mish: 1-6                              [-1, 40, 1, 18]           --
├─SwapLayer: 1-7                         [-1, 1, 40, 18]           --
├─Conv2d: 1-8                            [-1, 50, 33, 7]           4,850
├─BatchNorm2d: 1-9                       [-1, 50, 33, 7]           100
├─MaxPool2d: 1-10                        [-1, 50, 11, 2]           --
├─Mish: 1-11                             [-1, 50, 11, 2]           --
├─Conv2d: 1-12                           [-1, 50, 9, 2]            7,550
├─BatchNorm2d: 1-13                      [-1, 50, 9, 2]            100
├─MaxPool2d: 1-14                        [-1, 50, 9, 1]            --
├─Mish: 1-15                             [-1, 50, 9, 1]            --
├─Sequential: 1-16                       [-1, 2]                   --
|    └─Linear: 2-1                       [-1, 1024]                461,824
|    └─Mish: 2-2                         [-1, 1024]                --
|    └─Dropout: 2-3                      [-1, 1024]                --
|    └─Linear: 2-4                       [-1, 256]                 262,400
|    └─Mish: 2-5                         [-1, 256]                 --
|    └─Dropout: 2-6                      [-1, 256]                 --
|    └─Linear: 2-7                       [-1, 2]                   514
├─Sequential: 1-17                       [-1, 3]                   --
|    └─Linear: 2-8                       [-1, 1024]                461,824
|    └─Mish: 2-9                         [-1, 1024]                --
|    └─Dropout: 2-10                     [-1, 1024]                --
|    └─Linear: 2-11                      [-1, 256]                 262,400
|    └─Mish: 2-12                        [-1, 256]                 --
|    └─Dropout: 2-13                     [-1, 256]                 --
|    └─Linear: 2-14                      [-1, 3]                   771
==========================================================================================
Total params: 1,469,853
Trainable params: 1,469,853
Non-trainable params: 0
Total mult-adds (M): 4.83
==========================================================================================
Input size (MB): 0.15
Forward/backward pass size (MB): 0.30
Params size (MB): 5.61
Estimated Total Size (MB): 6.05
==========================================================================================

----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457      0.375     4.0      33.5   0.711472    0.82513   1.53545

----------------------------------------


Validation score decreased (inf --> -0.521537).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#4_epoch#000.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#4_epoch#000.pth

remove: path should be string, bytes or os.PathLike, not NoneType

----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757

----------------------------------------


Validation score decreased (-0.521537 --> -0.579546).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#4_epoch#001.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#4_epoch#001.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#4_epoch#000.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#4_epoch#000.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252

----------------------------------------


Validation score decreased (-0.579546 --> -0.598567).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#4_epoch#003.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#4_epoch#003.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#4_epoch#001.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#4_epoch#001.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636

----------------------------------------


EarlyStopping counter: 2 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235

----------------------------------------


EarlyStopping counter: 3 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785

----------------------------------------


EarlyStopping counter: 4 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661

----------------------------------------


EarlyStopping counter: 5 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680
9         0.560835        1.0     4.0     612.0   0.401271        0.0  0.359212

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661
9         0.562881   1.000000     4.0     645.5   0.659541   0.000624  0.591270

----------------------------------------


EarlyStopping counter: 6 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680
9         0.560835        1.0     4.0     612.0   0.401271        0.0  0.359212
10        0.586878        1.0     4.0     680.0   0.391907        0.0  0.342663

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661
9         0.562881   1.000000     4.0     645.5   0.659541   0.000624  0.591270
10        0.578165   1.000000     4.0     713.5   0.654791   0.000430  0.575979

----------------------------------------


EarlyStopping counter: 7 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680
9         0.560835        1.0     4.0     612.0   0.401271        0.0  0.359212
10        0.586878        1.0     4.0     680.0   0.391907        0.0  0.342663
11        0.575233        1.0     4.0     748.0   0.403098        0.0  0.344412

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661
9         0.562881   1.000000     4.0     645.5   0.659541   0.000624  0.591270
10        0.578165   1.000000     4.0     713.5   0.654791   0.000430  0.575979
11        0.565325   1.000000     4.0     781.5   0.653796   0.000348  0.563970

----------------------------------------


EarlyStopping counter: 8 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680
9         0.560835        1.0     4.0     612.0   0.401271        0.0  0.359212
10        0.586878        1.0     4.0     680.0   0.391907        0.0  0.342663
11        0.575233        1.0     4.0     748.0   0.403098        0.0  0.344412
12        0.575649        1.0     4.0     816.0   0.404320        0.0  0.337463

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661
9         0.562881   1.000000     4.0     645.5   0.659541   0.000624  0.591270
10        0.578165   1.000000     4.0     713.5   0.654791   0.000430  0.575979
11        0.565325   1.000000     4.0     781.5   0.653796   0.000348  0.563970
12        0.576374   1.000000     4.0     849.5   0.648621   0.000442  0.548311

----------------------------------------


EarlyStopping counter: 9 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680
9         0.560835        1.0     4.0     612.0   0.401271        0.0  0.359212
10        0.586878        1.0     4.0     680.0   0.391907        0.0  0.342663
11        0.575233        1.0     4.0     748.0   0.403098        0.0  0.344412
12        0.575649        1.0     4.0     816.0   0.404320        0.0  0.337463
13        0.563381        1.0     4.0     884.0   0.396382        0.0  0.321301

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661
9         0.562881   1.000000     4.0     645.5   0.659541   0.000624  0.591270
10        0.578165   1.000000     4.0     713.5   0.654791   0.000430  0.575979
11        0.565325   1.000000     4.0     781.5   0.653796   0.000348  0.563970
12        0.576374   1.000000     4.0     849.5   0.648621   0.000442  0.548311
13        0.582495   1.000000     4.0     917.5   0.648423   0.000335  0.536617

----------------------------------------


EarlyStopping counter: 10 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680
9         0.560835        1.0     4.0     612.0   0.401271        0.0  0.359212
10        0.586878        1.0     4.0     680.0   0.391907        0.0  0.342663
11        0.575233        1.0     4.0     748.0   0.403098        0.0  0.344412
12        0.575649        1.0     4.0     816.0   0.404320        0.0  0.337463
13        0.563381        1.0     4.0     884.0   0.396382        0.0  0.321301
14        0.574231        1.0     4.0     952.0   0.396260        0.0  0.311766

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661
9         0.562881   1.000000     4.0     645.5   0.659541   0.000624  0.591270
10        0.578165   1.000000     4.0     713.5   0.654791   0.000430  0.575979
11        0.565325   1.000000     4.0     781.5   0.653796   0.000348  0.563970
12        0.576374   1.000000     4.0     849.5   0.648621   0.000442  0.548311
13        0.582495   1.000000     4.0     917.5   0.648423   0.000335  0.536617
14        0.576282   1.000000     4.0     985.5   0.642890   0.000295  0.520184

----------------------------------------


EarlyStopping counter: 11 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680
9         0.560835        1.0     4.0     612.0   0.401271        0.0  0.359212
10        0.586878        1.0     4.0     680.0   0.391907        0.0  0.342663
11        0.575233        1.0     4.0     748.0   0.403098        0.0  0.344412
12        0.575649        1.0     4.0     816.0   0.404320        0.0  0.337463
13        0.563381        1.0     4.0     884.0   0.396382        0.0  0.321301
14        0.574231        1.0     4.0     952.0   0.396260        0.0  0.311766
15        0.593217        1.0     4.0    1020.0   0.392497        0.0  0.299537

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661
9         0.562881   1.000000     4.0     645.5   0.659541   0.000624  0.591270
10        0.578165   1.000000     4.0     713.5   0.654791   0.000430  0.575979
11        0.565325   1.000000     4.0     781.5   0.653796   0.000348  0.563970
12        0.576374   1.000000     4.0     849.5   0.648621   0.000442  0.548311
13        0.582495   1.000000     4.0     917.5   0.648423   0.000335  0.536617
14        0.576282   1.000000     4.0     985.5   0.642890   0.000295  0.520184
15        0.590979   1.000000     4.0    1053.5   0.631558   0.000224  0.498719

----------------------------------------


EarlyStopping counter: 12 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680
9         0.560835        1.0     4.0     612.0   0.401271        0.0  0.359212
10        0.586878        1.0     4.0     680.0   0.391907        0.0  0.342663
11        0.575233        1.0     4.0     748.0   0.403098        0.0  0.344412
12        0.575649        1.0     4.0     816.0   0.404320        0.0  0.337463
13        0.563381        1.0     4.0     884.0   0.396382        0.0  0.321301
14        0.574231        1.0     4.0     952.0   0.396260        0.0  0.311766
15        0.593217        1.0     4.0    1020.0   0.392497        0.0  0.299537
16        0.608426        1.0     4.0    1088.0   0.406372        0.0  0.300971

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661
9         0.562881   1.000000     4.0     645.5   0.659541   0.000624  0.591270
10        0.578165   1.000000     4.0     713.5   0.654791   0.000430  0.575979
11        0.565325   1.000000     4.0     781.5   0.653796   0.000348  0.563970
12        0.576374   1.000000     4.0     849.5   0.648621   0.000442  0.548311
13        0.582495   1.000000     4.0     917.5   0.648423   0.000335  0.536617
14        0.576282   1.000000     4.0     985.5   0.642890   0.000295  0.520184
15        0.590979   1.000000     4.0    1053.5   0.631558   0.000224  0.498719
16        0.605258   1.000000     4.0    1121.5   0.631248   0.000247  0.486644

----------------------------------------


Validation score decreased (-0.598567 --> -0.608426).  Saving model ...


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#4_epoch#016.pth


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#4_epoch#016.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/model_fold#4_epoch#003.pth


Removed /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/checkpoints/mtl_fold#4_epoch#003.pth


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680
9         0.560835        1.0     4.0     612.0   0.401271        0.0  0.359212
10        0.586878        1.0     4.0     680.0   0.391907        0.0  0.342663
11        0.575233        1.0     4.0     748.0   0.403098        0.0  0.344412
12        0.575649        1.0     4.0     816.0   0.404320        0.0  0.337463
13        0.563381        1.0     4.0     884.0   0.396382        0.0  0.321301
14        0.574231        1.0     4.0     952.0   0.396260        0.0  0.311766
15        0.593217        1.0     4.0    1020.0   0.392497        0.0  0.299537
16        0.608426        1.0     4.0    1088.0   0.406372        0.0  0.300971
17        0.583821        1.0     4.0    1156.0   0.397269        0.0  0.282294

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661
9         0.562881   1.000000     4.0     645.5   0.659541   0.000624  0.591270
10        0.578165   1.000000     4.0     713.5   0.654791   0.000430  0.575979
11        0.565325   1.000000     4.0     781.5   0.653796   0.000348  0.563970
12        0.576374   1.000000     4.0     849.5   0.648621   0.000442  0.548311
13        0.582495   1.000000     4.0     917.5   0.648423   0.000335  0.536617
14        0.576282   1.000000     4.0     985.5   0.642890   0.000295  0.520184
15        0.590979   1.000000     4.0    1053.5   0.631558   0.000224  0.498719
16        0.605258   1.000000     4.0    1121.5   0.631248   0.000247  0.486644
17        0.598242   1.000000     4.0    1189.5   0.630059   0.000161  0.473492

----------------------------------------


EarlyStopping counter: 1 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680
9         0.560835        1.0     4.0     612.0   0.401271        0.0  0.359212
10        0.586878        1.0     4.0     680.0   0.391907        0.0  0.342663
11        0.575233        1.0     4.0     748.0   0.403098        0.0  0.344412
12        0.575649        1.0     4.0     816.0   0.404320        0.0  0.337463
13        0.563381        1.0     4.0     884.0   0.396382        0.0  0.321301
14        0.574231        1.0     4.0     952.0   0.396260        0.0  0.311766
15        0.593217        1.0     4.0    1020.0   0.392497        0.0  0.299537
16        0.608426        1.0     4.0    1088.0   0.406372        0.0  0.300971
17        0.583821        1.0     4.0    1156.0   0.397269        0.0  0.282294
18        0.578628        1.0     4.0    1224.0   0.394714        0.0  0.269902

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661
9         0.562881   1.000000     4.0     645.5   0.659541   0.000624  0.591270
10        0.578165   1.000000     4.0     713.5   0.654791   0.000430  0.575979
11        0.565325   1.000000     4.0     781.5   0.653796   0.000348  0.563970
12        0.576374   1.000000     4.0     849.5   0.648621   0.000442  0.548311
13        0.582495   1.000000     4.0     917.5   0.648423   0.000335  0.536617
14        0.576282   1.000000     4.0     985.5   0.642890   0.000295  0.520184
15        0.590979   1.000000     4.0    1053.5   0.631558   0.000224  0.498719
16        0.605258   1.000000     4.0    1121.5   0.631248   0.000247  0.486644
17        0.598242   1.000000     4.0    1189.5   0.630059   0.000161  0.473492
18        0.596791   1.000000     4.0    1257.5   0.634050   0.000120  0.464560

----------------------------------------


EarlyStopping counter: 2 out of 50


----------------------------------------


Validation: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.521537        0.0     4.0       0.0   0.343583        0.0  0.343583
1         0.579546        1.0     4.0      68.0   0.376251        0.0  0.378972
2         0.552095        1.0     4.0     136.0   0.390575        0.0  0.391508
3         0.598567        1.0     4.0     204.0   0.393160        0.0  0.389892
4         0.570741        1.0     4.0     272.0   0.397049        0.0  0.388309
5         0.597357        1.0     4.0     340.0   0.404056        0.0  0.389072
6         0.587888        1.0     4.0     408.0   0.396668        0.0  0.376096
7         0.579513        1.0     4.0     476.0   0.385773        0.0  0.359046
8         0.564803        1.0     4.0     544.0   0.394175        0.0  0.359680
9         0.560835        1.0     4.0     612.0   0.401271        0.0  0.359212
10        0.586878        1.0     4.0     680.0   0.391907        0.0  0.342663
11        0.575233        1.0     4.0     748.0   0.403098        0.0  0.344412
12        0.575649        1.0     4.0     816.0   0.404320        0.0  0.337463
13        0.563381        1.0     4.0     884.0   0.396382        0.0  0.321301
14        0.574231        1.0     4.0     952.0   0.396260        0.0  0.311766
15        0.593217        1.0     4.0    1020.0   0.392497        0.0  0.299537
16        0.608426        1.0     4.0    1088.0   0.406372        0.0  0.300971
17        0.583821        1.0     4.0    1156.0   0.397269        0.0  0.282294
18        0.578628        1.0     4.0    1224.0   0.394714        0.0  0.269902
19        0.563974        1.0     4.0    1292.0   0.402319        0.0  0.264991

----------------------------------------


----------------------------------------


Training: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
0         0.524457   0.375000     4.0      33.5   0.711472   0.825130  1.535450
1         0.520525   0.970588     4.0     101.5   0.694180   0.139092  0.832757
2         0.528128   1.000000     4.0     169.5   0.686279   0.016752  0.697552
3         0.546333   1.000000     4.0     237.5   0.679270   0.005772  0.672252
4         0.553322   1.000000     4.0     305.5   0.670711   0.003091  0.652979
5         0.556005   1.000000     4.0     373.5   0.668268   0.001968  0.640636
6         0.553083   1.000000     4.0     441.5   0.668863   0.001411  0.631235
7         0.580879   1.000000     4.0     509.5   0.656786   0.001110  0.609785
8         0.578561   1.000000     4.0     577.5   0.656001   0.000805  0.598661
9         0.562881   1.000000     4.0     645.5   0.659541   0.000624  0.591270
10        0.578165   1.000000     4.0     713.5   0.654791   0.000430  0.575979
11        0.565325   1.000000     4.0     781.5   0.653796   0.000348  0.563970
12        0.576374   1.000000     4.0     849.5   0.648621   0.000442  0.548311
13        0.582495   1.000000     4.0     917.5   0.648423   0.000335  0.536617
14        0.576282   1.000000     4.0     985.5   0.642890   0.000295  0.520184
15        0.590979   1.000000     4.0    1053.5   0.631558   0.000224  0.498719
16        0.605258   1.000000     4.0    1121.5   0.631248   0.000247  0.486644
17        0.598242   1.000000     4.0    1189.5   0.630059   0.000161  0.473492
18        0.596791   1.000000     4.0    1257.5   0.634050   0.000120  0.464560
19        0.602105   1.000000     4.0    1325.5   0.629220   0.000140  0.448082

----------------------------------------


EarlyStopping counter: 3 out of 50


----------------------------------------


Test: (mean of batches)

         Bacc Diag  Bacc Subj  i_fold  i_global  Loss Diag  Loss Subj  Loss Tot
i_epoch                                                                        
16        0.579221        1.0     4.0    1156.0   0.689356        0.0   0.52909

----------------------------------------


Matplotilb has been configured as follows:
{'figure.dpi': 100, 'savefig.dpi': 300, 'figure.figsize': '(7.0, 7.0) [cm]', 'font.size': 7, 'axes.labelsize': 8, 'xtick.labelsize': 8, 'ytick.labelsize': 8, 'axes.titlesize': 8, 'legend.fontsize': 6, 'pdf.fonttype': 42, 'ps.fonttype': 42, 'axes.spines.top': True, 'axes.spines.right': True, 'xtick.major.size': 2.032, 'ytick.major.size': 2.032, 'xtick.major.width': 0.508, 'ytick.major.width': 0.508}.


Balanced ACC in fold#4 was 0.523


MCC in fold#4 was 0.072


Confusion Matrix in fold#4: 

            NoN-Ripple  Ripple
NoN-Ripple        2507     258
Ripple            1641     266


Classification Report for fold#4:

             NoN-Ripple    Ripple  balanced accuracy  macro avg  weighted avg
precision         0.604     0.508              0.523      0.556         0.565
recall            0.907     0.139              0.523      0.523         0.594
f1-score          0.725     0.219              0.523      0.472         0.519
sample size    2765.000  1907.000              0.523   4672.000      4672.000


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/fold#4/NoN-Ripple.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/fold#4/Ripple.csv


Matplotilb has been configured as follows:
{'figure.dpi': 300, 'savefig.dpi': 300, 'figure.figsize': '(32.4, 20.0) [cm]', 'font.size': 16, 'axes.labelsize': 16, 'xtick.labelsize': 16, 'ytick.labelsize': 16, 'axes.titlesize': 16, 'legend.fontsize': 'xx-small', 'pdf.fonttype': 42, 'ps.fonttype': 42, 'axes.spines.top': True, 'axes.spines.right': True}.


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/k-fold_mean_std/NoN-Ripple.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/csv/k-fold_mean_std/Ripple.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/default_global_conf.yaml


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/default_model_conf.yaml


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/default_dl_conf.yaml


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/merged_conf.yaml


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/balanced_acc.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/mcc.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/conf_mat/conf_mat.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/clf_report.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/micro.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/macro.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/figs/fold#0.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/figs/fold#1.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/figs/fold#2.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/figs/fold#3.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/roc/figs/fold#4.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/pre_rec/micro.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/pre_rec/macro.csv


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/pre_rec/figs/fold#0.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/pre_rec/figs/fold#1.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/pre_rec/figs/fold#2.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/pre_rec/figs/fold#3.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/pre_rec/figs/fold#4.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/learning_curve/fold#0.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/learning_curve/fold#1.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/learning_curve/fold#2.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/learning_curve/fold#3.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/learning_curve/fold#4.png


Matplotilb has been configured as follows:
{'figure.dpi': 100, 'savefig.dpi': 300, 'figure.figsize': '(30, 30) [cm]', 'font.size': 16, 'axes.labelsize': 16, 'xtick.labelsize': 16, 'ytick.labelsize': 16, 'axes.titlesize': 16, 'legend.fontsize': 'xx-small', 'pdf.fonttype': 42, 'ps.fonttype': 42, 'axes.spines.top': True, 'axes.spines.right': True, 'xtick.major.size': 2.032, 'ytick.major.size': 2.032, 'xtick.major.width': 0.508, 'ytick.major.width': 0.508}.


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/conf_mat/figs/fold#0.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/conf_mat/figs/fold#1.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/conf_mat/figs/fold#2.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/conf_mat/figs/fold#3.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/conf_mat/figs/fold#4.png


Saved to: /home/ywatanabe/proj/human_ripple_EEG/train/MNet/ywatanabe/NoN-Ripple_vs_Ripple/_MNet_100_WindowSize-0.5-sec_MaxEpochs_20_2022-1101-1852/seg-level/conf_mat/figs/5-fold_cv_overall-sum.png

